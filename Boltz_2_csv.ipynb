{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JKourelis/Colab_Boltz-2/blob/main/Boltz_2_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqNojEcK_HM1"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/jwohlwend/boltz/main/docs/boltz2_title.png\" height=\"200\" align=\"right\" style=\"height:240px\">\n",
        "\n",
        "## Boltz-2: Democratizing Biomolecular Interaction Modeling\n",
        "\n",
        "Easy to use protein structure and binding affinity prediction using [Boltz-2](https://doi.org/10.1101/2025.06.14.659707). Boltz-2 is a biomolecular foundation model that jointly models complex structures and binding affinities, approaching [AlphaFold3](https://www.nature.com/articles/s41586-024-07487-w) accuracy while running 1000x faster than physics-based methods.\n",
        "\n",
        "**Key Features:**\n",
        "- **Structure Prediction**: Protein, DNA, RNA, and ligand complexes with AlphaFold3-level accuracy\n",
        "- **Binding Affinity**: First deep learning model to approach FEP accuracy for drug discovery\n",
        "- **Open Source**: MIT license for academic and commercial use\n",
        "- **Fast**: 1000x faster than traditional physics-based methods\n",
        "\n",
        "**Usage Options:**\n",
        "1. **Manual Input**: Enter sequences directly in the configuration boxes below\n",
        "2. **FASTA Upload**: Upload FASTA files for batch processing\n",
        "\n",
        "**Repository:**\n",
        "- [Boltz-2 Colab Repository](https://github.com/JKourelis/Colab_Boltz-2)\n",
        "\n",
        "**Citations:**\n",
        "\n",
        "[Wohlwend J, Corso G, Passaro S, et al. Boltz-1: Democratizing Biomolecular Interaction Modeling. *bioRxiv*, 2024](https://doi.org/10.1101/2024.11.19.624167)\n",
        "\n",
        "[Passaro S, Corso G, Wohlwend J, et al. Boltz-2: Towards Accurate and Efficient Binding Affinity Prediction. *bioRxiv*, 2025](https://doi.org/10.1101/2025.06.14.659707)\n",
        "\n",
        "If using automatic MSA generation: [Mirdita M, Sch√ºtze K, Moriwaki Y, et al. ColabFold: making protein folding accessible to all. *Nature Methods*, 2022](https://doi.org/10.1038/s41592-022-01488-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Lo35v692ER",
        "outputId": "5dd1704d-9924-4580-d3ea-c7acf504578e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "INSTALLING BOLTZ-2\n",
            "============================================================\n",
            "‚úÖ CUDA Version: 12.4\n",
            "   Driver CUDA: 12.4\n",
            "\n",
            "üì¶ Selected PyTorch configuration:\n",
            "   PyTorch: 2.6.0\n",
            "   CUDA wheels: cu124\n",
            "   Rationale: cu124 compatible with CUDA 12.3-12.5\n",
            "\n",
            "============================================================\n",
            "[Cleanup]\n",
            "OK\n",
            "\n",
            "============================================================\n",
            "[1/4] NumPy 1.26.4 - FIRST (Colab compatibility)\n",
            "[Installing numpy==1.26.4]\n",
            "OK\n",
            "   ‚úÖ Verified: NumPy 1.26.4\n",
            "\n",
            "============================================================\n",
            "[2/4] PyTorch 2.6.0 (cu124)\n",
            "[Installing PyTorch 2.6.0]\n",
            "OK\n",
            "\n",
            "============================================================\n",
            "[3/4] Lightning stack\n",
            "[Installing Lightning]\n",
            "OK\n",
            "\n",
            "============================================================\n",
            "[4/4] Boltz-2 (+ dependencies)\n",
            "   Note: cuequivariance packages managed by Boltz\n",
            "[Installing Boltz-2]\n",
            "OK\n",
            "\n",
            "============================================================\n",
            "INSTALLING PERMANENT SYS.PATH FIX\n",
            "============================================================\n",
            "   ‚úÖ Created /usr/local/lib/python3.12/dist-packages/sitecustomize.py\n",
            "   This fixes sys.path on EVERY future kernel start\n",
            "   ‚úÖ Created /root/.ipython/profile_default/startup/00-fix_syspath.py\n",
            "   Backup fix for IPython kernels\n",
            "\n",
            "============================================================\n",
            "APPLYING FIX TO CURRENT KERNEL\n",
            "============================================================\n",
            "   ‚úÖ Removed /env/python from sys.path\n",
            "   ‚úÖ Cleared PYTHONPATH environment variable\n",
            "   ‚úÖ Cleared 86 cached modules\n",
            "\n",
            "============================================================\n",
            "VERIFICATION\n",
            "============================================================\n",
            "\n",
            "[Testing imports in current kernel]\n",
            "   ‚úÖ NumPy 1.26.4\n",
            "   ‚úÖ Pandas 2.3.3\n",
            "\n",
            "   üéâ All imports working!\n",
            "\n",
            "============================================================\n",
            "CUEQUIVARIANCE KERNEL PREFLIGHT TEST\n",
            "============================================================\n",
            "‚úÖ PyTorch: 2.6.0+cu124\n",
            "‚úÖ CUDA available: True\n",
            "‚úÖ CUDA version: 12.4\n",
            "‚úÖ GPU: NVIDIA A100-SXM4-40GB\n",
            "‚ö†Ô∏è  cuequivariance-torch not found: No module named 'cuequivariance_torch'\n",
            "\n",
            "‚ùå KERNEL TEST FAILED\n",
            "   cuEquivariance kernels are NOT available\n",
            "   Will run Boltz-2 WITH --no_kernels flag\n",
            "   Performance penalty: ~12 seconds per prediction\n",
            "\n",
            "üîß Flag stored: use_no_kernels = True\n",
            "\n",
            "============================================================\n",
            "INSTALLED PACKAGE VERSIONS\n",
            "============================================================\n",
            "\n",
            "üìã Core packages:\n",
            "   numpy==1.26.4\n",
            "   pandas==2.3.3\n",
            "   scipy==1.13.1\n",
            "   torch==2.6.0+cu124\n",
            "   torchvision==0.21.0+cu124\n",
            "   torchaudio==2.6.0+cu124\n",
            "   pytorch-lightning==2.5.0\n",
            "   torchmetrics==1.4.0\n",
            "   boltz==2.2.1\n",
            "\n",
            "üìÑ Saving complete requirements.txt...\n",
            "   ‚úÖ Saved to: /content/requirements_boltz.txt\n",
            "\n",
            "============================================================\n",
            "‚úÖ BOLTZ-2 INSTALLATION COMPLETE\n",
            "============================================================\n",
            "Next: Run Cell 2 to set up CSV processor\n",
            "CPU times: user 2.07 s, sys: 1.35 s, total: 3.41 s\n",
            "Wall time: 3min 53s\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 1: Install Boltz-2 with cuEquivariance Kernel Test\n",
        "%%time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Restart marker to handle Colab Feb 2025 NumPy issue\n",
        "restart_marker = \"/content/.boltz_numpy_restart\"\n",
        "is_post_restart = os.path.exists(restart_marker)\n",
        "\n",
        "def run_cmd(cmd, desc):\n",
        "    \"\"\"Execute command with output suppression unless error\"\"\"\n",
        "    print(f\"[{desc}]\")\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"FAILED: {result.stderr[:300]}\")\n",
        "        return False\n",
        "    print(\"OK\")\n",
        "    return True\n",
        "\n",
        "def get_cuda_version():\n",
        "    \"\"\"Detect CUDA version from nvidia-smi\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            match = re.search(r'CUDA Version: (\\d+\\.\\d+)', result.stdout)\n",
        "            if match:\n",
        "                version = match.group(1)\n",
        "                major = int(version.split('.')[0])\n",
        "                minor = int(version.split('.')[1])\n",
        "                return major, minor, version\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Could not detect CUDA version: {e}\")\n",
        "    return None, None, None\n",
        "\n",
        "def test_cuequivariance_kernels():\n",
        "    \"\"\"Test if cuEquivariance triangle kernels are available\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"CUEQUIVARIANCE KERNEL PREFLIGHT TEST\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "        print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
        "        print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
        "            print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå PyTorch check failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Test cuequivariance-torch import\n",
        "    try:\n",
        "        import cuequivariance_torch\n",
        "        print(f\"‚úÖ cuequivariance-torch installed\")\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ö†Ô∏è  cuequivariance-torch not found: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Test cuequivariance-ops-torch-cu12 import\n",
        "    try:\n",
        "        import cuequivariance_ops_torch\n",
        "        print(f\"‚úÖ cuequivariance-ops-torch-cu12 installed\")\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ö†Ô∏è  cuequivariance-ops-torch-cu12 not found: {e}\")\n",
        "        return False\n",
        "\n",
        "    # CRITICAL TEST: triangle_multiplicative_update\n",
        "    try:\n",
        "        from cuequivariance_ops_torch.triangle import triangle_multiplicative_update\n",
        "        print(f\"‚úÖ triangle_multiplicative_update import: SUCCESS\")\n",
        "\n",
        "        if callable(triangle_multiplicative_update):\n",
        "            print(f\"‚úÖ triangle_multiplicative_update is callable\")\n",
        "        else:\n",
        "            print(f\"‚ùå triangle_multiplicative_update exists but is not callable\")\n",
        "            return False\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ùå triangle_multiplicative_update import FAILED: {e}\")\n",
        "        print(f\"   This error requires --no_kernels flag\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error testing triangle kernels: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Test triangle_attention_update\n",
        "    try:\n",
        "        from cuequivariance_ops_torch.triangle import triangle_attention_update\n",
        "        print(f\"‚úÖ triangle_attention_update import: SUCCESS\")\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ö†Ô∏è  triangle_attention_update import failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    return True\n",
        "\n",
        "if not is_post_restart:\n",
        "    # PRE-RESTART: PREFLIGHT CHECKS\n",
        "    print(\"=\" * 60)\n",
        "    print(\"PREFLIGHT CHECKS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Check GPU\n",
        "    cuda_major, cuda_minor, cuda_version = get_cuda_version()\n",
        "    if cuda_major is None:\n",
        "        print(\"‚ùå No CUDA detected - cannot proceed\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"‚úÖ CUDA Version: {cuda_version}\")\n",
        "    print(f\"   Driver CUDA: {cuda_major}.{cuda_minor}\")\n",
        "\n",
        "    # Check GPU type\n",
        "    result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'],\n",
        "                          capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        gpu_name = result.stdout.strip()\n",
        "        print(f\"‚úÖ GPU: {gpu_name}\")\n",
        "\n",
        "    # Map CUDA version to PyTorch (SHOW PLAN BEFORE RESTART)\n",
        "    if cuda_major == 12:\n",
        "        if cuda_minor <= 1:\n",
        "            pytorch_cuda = \"cu121\"\n",
        "            pytorch_version = \"2.5.1\"\n",
        "            reason = \"cu121 compatible with CUDA 12.0-12.1 (PyTorch 2.5.1 max)\"\n",
        "        elif cuda_minor <= 2:\n",
        "            pytorch_cuda = \"cu121\"\n",
        "            pytorch_version = \"2.5.1\"\n",
        "            reason = \"cu121 forward compatible with CUDA 12.2 (PyTorch 2.5.1)\"\n",
        "        elif cuda_minor <= 5:\n",
        "            pytorch_cuda = \"cu124\"\n",
        "            pytorch_version = \"2.6.0\"\n",
        "            reason = \"cu124 compatible with CUDA 12.3-12.5 (PyTorch 2.6.0)\"\n",
        "        elif cuda_minor <= 7:\n",
        "            pytorch_cuda = \"cu126\"\n",
        "            pytorch_version = \"2.6.0\"\n",
        "            reason = \"cu126 compatible with CUDA 12.6-12.7 (PyTorch 2.6.0)\"\n",
        "        else:\n",
        "            pytorch_cuda = \"cu128\"\n",
        "            pytorch_version = \"2.7.0\"\n",
        "            reason = \"cu128 for CUDA 12.8+ (PyTorch 2.7.0)\"\n",
        "    elif cuda_major == 11:\n",
        "        pytorch_cuda = \"cu118\"\n",
        "        pytorch_version = \"2.6.0\"\n",
        "        reason = \"cu118 for CUDA 11.x (PyTorch 2.6.0)\"\n",
        "    else:\n",
        "        print(f\"‚ùå Unsupported CUDA major version: {cuda_major}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"\\nüì¶ Selected PyTorch configuration:\")\n",
        "    print(f\"   PyTorch: {pytorch_version}\")\n",
        "    print(f\"   CUDA wheels: {pytorch_cuda}\")\n",
        "    print(f\"   Rationale: {reason}\")\n",
        "\n",
        "    # Check pre-loaded NumPy\n",
        "    result = subprocess.run([sys.executable, \"-c\", \"import numpy; print(numpy.__version__)\"],\n",
        "                          capture_output=True, text=True)\n",
        "    if result.returncode == 0:\n",
        "        numpy_ver = result.stdout.strip()\n",
        "        print(f\"\\n‚ö†Ô∏è  Pre-loaded NumPy: {numpy_ver}\")\n",
        "        if numpy_ver.startswith('2.'):\n",
        "            print(\"   ‚Üí Colab Feb 2025 issue detected\")\n",
        "            print(\"   ‚Üí NumPy 2.x must be cleared from memory\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"RESTARTING RUNTIME TO CLEAR NUMPY 2.X\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Runtime will restart in 2 seconds\")\n",
        "    print(\"After restart: Run this cell again to install\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    with open(restart_marker, \"w\") as f:\n",
        "        f.write(\"restarted\")\n",
        "\n",
        "    import time\n",
        "    time.sleep(2)\n",
        "    os.kill(os.getpid(), 9)\n",
        "\n",
        "else:\n",
        "    # POST-RESTART: INSTALLATION\n",
        "    print(\"=\" * 60)\n",
        "    print(\"INSTALLING BOLTZ-2\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Detect CUDA version\n",
        "    cuda_major, cuda_minor, cuda_version = get_cuda_version()\n",
        "\n",
        "    if cuda_major is None:\n",
        "        print(\"‚ùå No CUDA detected - cannot proceed\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"‚úÖ CUDA Version: {cuda_version}\")\n",
        "    print(f\"   Driver CUDA: {cuda_major}.{cuda_minor}\")\n",
        "\n",
        "    # Map CUDA version to PyTorch wheel\n",
        "    if cuda_major == 12:\n",
        "        if cuda_minor <= 1:\n",
        "            pytorch_cuda = \"cu121\"\n",
        "            pytorch_version = \"2.5.1\"\n",
        "            reason = \"cu121 compatible with CUDA 12.0-12.1\"\n",
        "        elif cuda_minor <= 2:\n",
        "            pytorch_cuda = \"cu121\"\n",
        "            pytorch_version = \"2.5.1\"\n",
        "            reason = \"cu121 forward compatible with CUDA 12.2\"\n",
        "        elif cuda_minor <= 5:\n",
        "            pytorch_cuda = \"cu124\"\n",
        "            pytorch_version = \"2.6.0\"\n",
        "            reason = \"cu124 compatible with CUDA 12.3-12.5\"\n",
        "        elif cuda_minor <= 7:\n",
        "            pytorch_cuda = \"cu126\"\n",
        "            pytorch_version = \"2.6.0\"\n",
        "            reason = \"cu126 compatible with CUDA 12.6-12.7\"\n",
        "        else:\n",
        "            pytorch_cuda = \"cu128\"\n",
        "            pytorch_version = \"2.7.0\"\n",
        "            reason = \"cu128 for CUDA 12.8+\"\n",
        "    elif cuda_major == 11:\n",
        "        pytorch_cuda = \"cu118\"\n",
        "        pytorch_version = \"2.6.0\"\n",
        "        reason = \"cu118 for CUDA 11.x\"\n",
        "    else:\n",
        "        print(f\"‚ùå Unsupported CUDA major version: {cuda_major}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"\\nüì¶ Selected PyTorch configuration:\")\n",
        "    print(f\"   PyTorch: {pytorch_version}\")\n",
        "    print(f\"   CUDA wheels: {pytorch_cuda}\")\n",
        "    print(f\"   Rationale: {reason}\")\n",
        "\n",
        "    # Clean existing installations\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"[Cleanup]\")\n",
        "    cleanup_packages = [\n",
        "        'torch', 'torchvision', 'torchaudio',\n",
        "        'pytorch-lightning', 'torchmetrics',\n",
        "        'boltz', 'numpy', 'pandas'\n",
        "    ]\n",
        "    subprocess.run(\n",
        "        f\"{sys.executable} -m pip uninstall {' '.join(cleanup_packages)} -y\",\n",
        "        shell=True, capture_output=True\n",
        "    )\n",
        "    print(\"OK\")\n",
        "\n",
        "    # Install NumPy FIRST\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"[1/4] NumPy 1.26.4 - FIRST (Colab compatibility)\")\n",
        "    if not run_cmd(\n",
        "        f\"{sys.executable} -m pip install --no-cache-dir 'numpy==1.26.4'\",\n",
        "        \"Installing numpy==1.26.4\"\n",
        "    ):\n",
        "        print(\"‚ùå NumPy installation failed\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Verify NumPy\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-c\", \"import numpy; print(numpy.__version__)\"],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "    if '1.26' not in result.stdout:\n",
        "        print(f\"‚ùå NumPy version wrong: {result.stdout.strip()}\")\n",
        "        sys.exit(1)\n",
        "    print(f\"   ‚úÖ Verified: NumPy {result.stdout.strip()}\")\n",
        "\n",
        "    # Install PyTorch\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"[2/4] PyTorch {pytorch_version} ({pytorch_cuda})\")\n",
        "    pytorch_url = f\"https://download.pytorch.org/whl/{pytorch_cuda}\"\n",
        "    if not run_cmd(\n",
        "        f\"{sys.executable} -m pip install torch=={pytorch_version} torchvision torchaudio --index-url {pytorch_url}\",\n",
        "        f\"Installing PyTorch {pytorch_version}\"\n",
        "    ):\n",
        "        print(\"‚ùå PyTorch installation failed\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Install pytorch-lightning\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"[3/4] Lightning stack\")\n",
        "    if not run_cmd(\n",
        "        f\"{sys.executable} -m pip install pytorch-lightning==2.5.0 torchmetrics==1.4.0\",\n",
        "        \"Installing Lightning\"\n",
        "    ):\n",
        "        print(\"‚ùå Lightning installation failed\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Install Boltz\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"[4/4] Boltz-2 (+ dependencies)\")\n",
        "    print(\"   Note: cuequivariance packages managed by Boltz\")\n",
        "    if not run_cmd(\n",
        "        f\"{sys.executable} -m pip install boltz\",\n",
        "        \"Installing Boltz-2\"\n",
        "    ):\n",
        "        print(\"‚ùå Boltz installation failed\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # PERMANENT FIX: Create sitecustomize.py\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INSTALLING PERMANENT SYS.PATH FIX\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    sitecustomize_content = \"\"\"# Colab Feb 2025 NumPy priority fix\n",
        "import sys\n",
        "import os\n",
        "\n",
        "if '/env/python' in sys.path:\n",
        "    sys.path.remove('/env/python')\n",
        "\n",
        "if 'PYTHONPATH' in os.environ:\n",
        "    del os.environ['PYTHONPATH']\n",
        "\"\"\"\n",
        "\n",
        "    sitecustomize_path = \"/usr/local/lib/python3.12/dist-packages/sitecustomize.py\"\n",
        "    with open(sitecustomize_path, \"w\") as f:\n",
        "        f.write(sitecustomize_content)\n",
        "\n",
        "    print(f\"   ‚úÖ Created {sitecustomize_path}\")\n",
        "    print(\"   This fixes sys.path on EVERY future kernel start\")\n",
        "\n",
        "    # IPython startup script\n",
        "    ipython_startup_dir = \"/root/.ipython/profile_default/startup\"\n",
        "    os.makedirs(ipython_startup_dir, exist_ok=True)\n",
        "\n",
        "    ipython_fix_path = os.path.join(ipython_startup_dir, \"00-fix_syspath.py\")\n",
        "    with open(ipython_fix_path, \"w\") as f:\n",
        "        f.write(sitecustomize_content)\n",
        "\n",
        "    print(f\"   ‚úÖ Created {ipython_fix_path}\")\n",
        "    print(\"   Backup fix for IPython kernels\")\n",
        "\n",
        "    # APPLY FIX TO CURRENT KERNEL\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"APPLYING FIX TO CURRENT KERNEL\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if '/env/python' in sys.path:\n",
        "        sys.path.remove('/env/python')\n",
        "        print(\"   ‚úÖ Removed /env/python from sys.path\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ /env/python not in sys.path\")\n",
        "\n",
        "    if 'PYTHONPATH' in os.environ:\n",
        "        del os.environ['PYTHONPATH']\n",
        "        print(\"   ‚úÖ Cleared PYTHONPATH environment variable\")\n",
        "\n",
        "    # Clear cached imports\n",
        "    modules_to_clear = [key for key in list(sys.modules.keys())\n",
        "                       if key.startswith(('numpy', 'pandas', 'np', 'pd'))]\n",
        "    for mod in modules_to_clear:\n",
        "        del sys.modules[mod]\n",
        "\n",
        "    if modules_to_clear:\n",
        "        print(f\"   ‚úÖ Cleared {len(modules_to_clear)} cached modules\")\n",
        "\n",
        "    # VERIFICATION IN CURRENT KERNEL\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"VERIFICATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"\\n[Testing imports in current kernel]\")\n",
        "    try:\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "\n",
        "        print(f\"   ‚úÖ NumPy {np.__version__}\")\n",
        "        print(f\"   ‚úÖ Pandas {pd.__version__}\")\n",
        "\n",
        "        if not np.__version__.startswith('1.26'):\n",
        "            print(f\"   ‚ùå NumPy version wrong: expected 1.26.x, got {np.__version__}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        print(\"\\n   üéâ All imports working!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Import failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "    # CUEQUIVARIANCE KERNEL TEST\n",
        "    kernels_available = test_cuequivariance_kernels()\n",
        "\n",
        "    if kernels_available:\n",
        "        print(\"\\n‚úÖ KERNEL TEST PASSED\")\n",
        "        print(\"   cuEquivariance kernels are available\")\n",
        "        print(\"   Will run Boltz-2 WITHOUT --no_kernels flag\")\n",
        "        use_no_kernels = False\n",
        "    else:\n",
        "        print(\"\\n‚ùå KERNEL TEST FAILED\")\n",
        "        print(\"   cuEquivariance kernels are NOT available\")\n",
        "        print(\"   Will run Boltz-2 WITH --no_kernels flag\")\n",
        "        print(\"   Performance penalty: ~12 seconds per prediction\")\n",
        "        use_no_kernels = True\n",
        "\n",
        "    # Store result for execution cell\n",
        "    if 'global_settings' not in globals():\n",
        "        global_settings = {}\n",
        "    global_settings['use_no_kernels'] = use_no_kernels\n",
        "    global_settings['kernels_tested'] = True\n",
        "\n",
        "    print(f\"\\nüîß Flag stored: use_no_kernels = {use_no_kernels}\")\n",
        "\n",
        "    # Show installed versions\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INSTALLED PACKAGE VERSIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"list\", \"--format=freeze\"],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "\n",
        "    all_packages = result.stdout.strip().split('\\n')\n",
        "    relevant = [\n",
        "        'numpy', 'pandas', 'scipy',\n",
        "        'torch', 'torchvision', 'torchaudio',\n",
        "        'pytorch-lightning', 'torchmetrics',\n",
        "        'boltz', 'cuequivariance-torch',\n",
        "        'cuequivariance-ops-torch-cu11',\n",
        "        'cuequivariance-ops-torch-cu12'\n",
        "    ]\n",
        "\n",
        "    print(\"\\nüìã Core packages:\")\n",
        "    for pkg in relevant:\n",
        "        for line in all_packages:\n",
        "            if line.lower().startswith(pkg.lower() + '=='):\n",
        "                print(f\"   {line}\")\n",
        "                break\n",
        "        else:\n",
        "            for line in all_packages:\n",
        "                if pkg.lower().replace('-', '_') in line.lower():\n",
        "                    print(f\"   {line}\")\n",
        "                    break\n",
        "\n",
        "    # Save complete requirements.txt\n",
        "    print(\"\\nüìÑ Saving complete requirements.txt...\")\n",
        "    with open(\"/content/requirements_boltz.txt\", \"w\") as f:\n",
        "        f.write(f\"# Boltz-2 Installation - CUDA {cuda_version}\\n\")\n",
        "        f.write(f\"# PyTorch {pytorch_version} ({pytorch_cuda})\\n\\n\")\n",
        "        f.write(result.stdout)\n",
        "    print(\"   ‚úÖ Saved to: /content/requirements_boltz.txt\")\n",
        "\n",
        "    # Cleanup and mark ready\n",
        "    os.remove(restart_marker)\n",
        "    with open(\"/content/BOLTZ_READY\", \"w\") as f:\n",
        "        f.write(\"Ready\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ BOLTZ-2 INSTALLATION COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Next: Run Cell 2 to set up CSV processor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwUc04CG-NzL",
        "outputId": "eab200b4-fa49-4150-b494-85599e7577a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Initializing Boltz CSV Processor...\n",
            "‚úÖ Using embedded reference data: 79 entries\n",
            "‚úÖ Processor ready\n",
            "üìã Reference data includes:\n",
            "   ‚Ä¢ 15 PTM types\n",
            "   ‚Ä¢ 24 ligand types\n",
            "   ‚Ä¢ 11 ion types\n",
            "   ‚Ä¢ 10 glycan types\n",
            "   ‚Ä¢ 8 DNA modification types\n",
            "   ‚Ä¢ 10 RNA modification types\n",
            "\n",
            "üí° Using embedded reference data (common PTMs, ligands, ions, glycans, DNA/RNA mods)\n",
            "   To use custom reference: upload file in Cell 3\n",
            "\n",
            "üìù Note: Chain IDs are assigned as A, B, C, D... sequentially\n",
            "   Sequence identity is preserved in job names\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 2: Boltz CSV Processor Setup\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from io import StringIO\n",
        "\n",
        "class BoltzJobProcessor:\n",
        "    \"\"\"Process CSV files for Boltz-2 batch predictions\"\"\"\n",
        "\n",
        "    EMBEDDED_REFERENCE = \"\"\"Type,CCD Code,Name,Target Residue,Molecular Formula,All Atom Count,Heavy Atom Count\n",
        "PTM,SEP,Phosphoserine,SER,C3H8NO6P,18,11\n",
        "PTM,TPO,Phosphothreonine,THR,C4H10NO6P,21,12\n",
        "PTM,PTR,Phosphotyrosine,TYR,C9H12NO6P,28,17\n",
        "PTM,MLY,N-Methyllysine,LYS,C7H16N2O2,27,11\n",
        "PTM,ALY,N-Acetyllysine,LYS,C8H16N2O3,29,13\n",
        "PTM,HYP,Hydroxyproline,PRO,C5H9NO3,18,9\n",
        "PTM,M3L,N-Trimethyllysine,LYS,C9H20N2O2,33,13\n",
        "PTM,MLZ,N-Methyllysine,LYS,C7H16N2O2,27,11\n",
        "PTM,CSD,Cysteine sulfinic acid,CYS,C3H7NO4S,16,9\n",
        "PTM,CSO,S-Hydroxycysteine,CYS,C3H7NO3S,15,8\n",
        "PTM,CGU,Gamma-carboxyglutamic acid,GLU,C5H7NO6,21,12\n",
        "PTM,FME,N-Formylmethionine,MET,C6H11NO3S,22,11\n",
        "PTM,NEP,N-(phosphonoethyl)isoleucine,ILE,C8H18NO5P,32,15\n",
        "PTM,HIC,4-Methyl-histidine,HIS,C7H11N3O2,23,12\n",
        "PTM,CAS,S-(dimethylarsenic)cysteine,CYS,C5H11AsNO2S,20,9\n",
        "Ligand,ATP,Adenosine triphosphate,NA,C10H16N5O13P3,47,31\n",
        "Ligand,ADP,Adenosine diphosphate,NA,C10H15N5O10P2,42,27\n",
        "Ligand,AMP,Adenosine monophosphate,NA,C10H14N5O7P,37,23\n",
        "Ligand,GTP,Guanosine triphosphate,NA,C10H16N5O14P3,48,32\n",
        "Ligand,GDP,Guanosine diphosphate,NA,C10H15N5O11P2,43,28\n",
        "Ligand,GMP,Guanosine monophosphate,NA,C10H14N5O8P,38,24\n",
        "Ligand,CTP,Cytidine triphosphate,NA,C9H16N3O14P3,45,29\n",
        "Ligand,CDP,Cytidine diphosphate,NA,C9H15N3O11P2,40,25\n",
        "Ligand,UTP,Uridine triphosphate,NA,C9H15N2O15P3,44,29\n",
        "Ligand,UDP,Uridine diphosphate,NA,C9H14N2O12P2,39,25\n",
        "Ligand,NAD,Nicotinamide adenine dinucleotide,NA,C21H27N7O14P2,71,44\n",
        "Ligand,NAP,NADP,NA,C21H28N7O17P3,86,55\n",
        "Ligand,FAD,Flavin adenine dinucleotide,NA,C27H33N9O15P2,91,53\n",
        "Ligand,FMN,Flavin mononucleotide,NA,C17H21N4O9P,52,31\n",
        "Ligand,HEM,Heme,NA,C34H32FeN4O4,75,43\n",
        "Ligand,SAH,S-Adenosyl-L-homocysteine,NA,C14H20N6O5S,46,26\n",
        "Ligand,SAM,S-Adenosyl-L-methionine,NA,C15H22N6O5S,49,27\n",
        "Ligand,COA,Coenzyme A,NA,C21H36N7O16P3S,90,57\n",
        "Ligand,ACO,Acetyl coenzyme A,NA,C23H38N7O17P3S,99,61\n",
        "Ligand,PLP,Pyridoxal-5-phosphate,NA,C8H10NO6P,25,16\n",
        "Ligand,TPP,Thiamine diphosphate,NA,C12H19N4O7P2S,45,25\n",
        "Ligand,BTN,Biotin,NA,C10H16N2O3S,32,16\n",
        "Ligand,MTA,5-Methylthioadenosine,NA,C11H15N5O3S,35,20\n",
        "Ligand,THM,Thiamine,NA,C12H17ClN4OS,38,18\n",
        "Ion,MG,Magnesium ion,NA,Mg,1,1\n",
        "Ion,ZN,Zinc ion,NA,Zn,1,1\n",
        "Ion,CA,Calcium ion,NA,Ca,1,1\n",
        "Ion,FE,Iron ion,NA,Fe,1,1\n",
        "Ion,MN,Manganese ion,NA,Mn,1,1\n",
        "Ion,CU,Copper ion,NA,Cu,1,1\n",
        "Ion,CO,Cobalt ion,NA,Co,1,1\n",
        "Ion,NI,Nickel ion,NA,Ni,1,1\n",
        "Ion,K,Potassium ion,NA,K,1,1\n",
        "Ion,NA,Sodium ion,NA,Na,1,1\n",
        "Ion,CL,Chloride ion,NA,Cl,1,1\n",
        "Glycan,NAG,N-Acetyl-D-glucosamine,NA,C8H15NO6,30,15\n",
        "Glycan,MAN,D-Mannose,NA,C6H12O6,24,12\n",
        "Glycan,FUC,L-Fucose,NA,C6H12O5,23,11\n",
        "Glycan,GAL,D-Galactose,NA,C6H12O6,24,12\n",
        "Glycan,SIA,N-Acetylneuraminic acid,NA,C11H19NO9,40,21\n",
        "Glycan,GLC,D-Glucose,NA,C6H12O6,24,12\n",
        "Glycan,BMA,beta-D-Mannose,NA,C6H12O6,24,12\n",
        "Glycan,NDG,N-Acetyl-D-glucosamine,NA,C8H15NO6,30,15\n",
        "Glycan,A2G,N-Acetyl-D-galactosamine,NA,C8H15NO6,30,15\n",
        "Glycan,FUL,L-Fucose,NA,C6H12O5,23,11\n",
        "DNA_Mod,5MC,5-Methylcytosine,DC,C10H15N3O7P,36,21\n",
        "DNA_Mod,6MA,N6-Methyladenine,DA,C11H16N5O6P,39,23\n",
        "DNA_Mod,5HMC,5-Hydroxymethylcytosine,DC,C10H15N3O8P,37,22\n",
        "DNA_Mod,8OG,8-Oxoguanine,DG,C10H13N5O8P,37,24\n",
        "DNA_Mod,M2G,N2-Methylguanine,DG,C11H16N5O7P,40,24\n",
        "DNA_Mod,4MC,N4-Methylcytosine,DC,C10H15N3O7P,36,21\n",
        "DNA_Mod,1MA,1-Methyladenine,DA,C11H16N5O6P,39,23\n",
        "DNA_Mod,3MA,3-Methyladenine,DA,C11H16N5O6P,39,23\n",
        "RNA_Mod,PSU,Pseudouridine,U,C9H12N2O9P,33,21\n",
        "RNA_Mod,1MA,1-Methyladenosine,A,C11H15N5O7P,39,24\n",
        "RNA_Mod,7MG,7-Methylguanosine,G,C11H15N5O8P,40,25\n",
        "RNA_Mod,5MC,5-Methylcytidine,C,C10H15N3O8P,37,22\n",
        "RNA_Mod,2MA,2-Methyladenosine,A,C11H15N5O7P,39,24\n",
        "RNA_Mod,M2G,N2-Methylguanosine,G,C11H15N5O8P,40,25\n",
        "RNA_Mod,M7G,7-Methylguanosine,G,C11H15N5O8P,40,25\n",
        "RNA_Mod,M1A,1-Methyladenosine,A,C11H15N5O7P,39,24\n",
        "RNA_Mod,OMC,2'-O-Methylcytidine,C,C10H15N3O8P,37,22\n",
        "RNA_Mod,OMG,2'-O-Methylguanosine,G,C11H15N5O8P,40,25\"\"\"\n",
        "\n",
        "    def __init__(self, reference_csv: Optional[str] = None):\n",
        "        \"\"\"Initialize processor with reference data\"\"\"\n",
        "        if reference_csv:\n",
        "            self.reference_data = pd.read_csv(StringIO(reference_csv))\n",
        "        else:\n",
        "            self.reference_data = pd.read_csv(StringIO(self.EMBEDDED_REFERENCE))\n",
        "\n",
        "        self.ptm_lookup = self._create_lookup('PTM')\n",
        "        self.ligand_lookup = self._create_lookup('Ligand')\n",
        "        self.ion_lookup = self._create_lookup('Ion')\n",
        "        self.glycan_lookup = self._create_lookup('Glycan')\n",
        "        self.dna_mod_lookup = self._create_lookup('DNA_Mod')\n",
        "        self.rna_mod_lookup = self._create_lookup('RNA_Mod')\n",
        "\n",
        "        self.aa_3to1 = {\n",
        "            'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E', 'PHE': 'F',\n",
        "            'GLY': 'G', 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L',\n",
        "            'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN': 'Q', 'ARG': 'R',\n",
        "            'SER': 'S', 'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y'\n",
        "        }\n",
        "\n",
        "        self.amino_acids = set('ACDEFGHIKLMNPQRSTVWY')\n",
        "\n",
        "    def _create_lookup(self, type_name: str) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Create lookup dictionary for a specific type\"\"\"\n",
        "        type_data = self.reference_data[self.reference_data['Type'] == type_name]\n",
        "        lookup = {}\n",
        "        for _, row in type_data.iterrows():\n",
        "            if pd.notna(row['CCD Code']):\n",
        "                lookup[row['CCD Code']] = {\n",
        "                    'name': row['Name'],\n",
        "                    'target_residue': row['Target Residue'] if pd.notna(row['Target Residue']) else 'NA'\n",
        "                }\n",
        "        return lookup\n",
        "\n",
        "    def _validate_sequence_characters(self, sequence: str, seq_type: str) -> List[str]:\n",
        "        \"\"\"Validate sequence contains only allowed characters\"\"\"\n",
        "        errors = []\n",
        "        sequence = sequence.upper()\n",
        "\n",
        "        if seq_type.lower() == 'protein':\n",
        "            invalid_chars = []\n",
        "            for i, char in enumerate(sequence, 1):\n",
        "                if char.isalpha() and char not in self.amino_acids:\n",
        "                    invalid_chars.append(f\"{char}@{i}\")\n",
        "                elif not char.isalpha() and not char.isspace():\n",
        "                    invalid_chars.append(f\"{char}@{i}\")\n",
        "            if invalid_chars:\n",
        "                errors.append(f\"Invalid amino acids: {', '.join(invalid_chars[:10])}\" +\n",
        "                            (\"...\" if len(invalid_chars) > 10 else \"\"))\n",
        "\n",
        "        elif seq_type.lower() == 'dna':\n",
        "            valid_bases = set('ATCG')\n",
        "            invalid_chars = []\n",
        "            for i, char in enumerate(sequence, 1):\n",
        "                if char.isalpha() and char not in valid_bases:\n",
        "                    invalid_chars.append(f\"{char}@{i}\")\n",
        "                elif not char.isalpha() and not char.isspace():\n",
        "                    invalid_chars.append(f\"{char}@{i}\")\n",
        "            if invalid_chars:\n",
        "                errors.append(f\"Invalid DNA bases: {', '.join(invalid_chars[:10])}\" +\n",
        "                            (\"...\" if len(invalid_chars) > 10 else \"\"))\n",
        "\n",
        "        elif seq_type.lower() == 'rna':\n",
        "            valid_bases = set('AUCG')\n",
        "            invalid_chars = []\n",
        "            for i, char in enumerate(sequence, 1):\n",
        "                if char.isalpha() and char not in valid_bases:\n",
        "                    invalid_chars.append(f\"{char}@{i}\")\n",
        "                elif not char.isalpha() and not char.isspace():\n",
        "                    invalid_chars.append(f\"{char}@{i}\")\n",
        "            if invalid_chars:\n",
        "                errors.append(f\"Invalid RNA bases: {', '.join(invalid_chars[:10])}\" +\n",
        "                            (\"...\" if len(invalid_chars) > 10 else \"\"))\n",
        "\n",
        "        return errors\n",
        "\n",
        "    def _is_smiles(self, ligand_string: str) -> bool:\n",
        "        \"\"\"Check if string is likely a SMILES representation\"\"\"\n",
        "        if len(ligand_string) < 3:\n",
        "            return False\n",
        "        smiles_chars = set('[]()=#@+-\\\\/CNOPSFClBrI0123456789')\n",
        "        return any(char in smiles_chars for char in ligand_string)\n",
        "\n",
        "    def _remap_modification_chains(self, mod_string: str, name_mapping: Dict[str, str]) -> str:\n",
        "        \"\"\"Remap modification chain IDs from user names to A, B, C... format\"\"\"\n",
        "        if pd.isna(mod_string) or str(mod_string).strip() == '':\n",
        "            return mod_string\n",
        "\n",
        "        mod_string = str(mod_string).strip()\n",
        "\n",
        "        for old_name, new_chain_id in name_mapping.items():\n",
        "            mod_string = mod_string.replace(f\"{old_name}:\", f\"{new_chain_id}:\")\n",
        "\n",
        "        return mod_string\n",
        "\n",
        "    def _remap_contact_chains(self, contacts_string: str, name_mapping: Dict[str, str]) -> str:\n",
        "        \"\"\"Remap contact chain IDs from user names to A, B, C... format\"\"\"\n",
        "        if pd.isna(contacts_string) or str(contacts_string).strip() == '':\n",
        "            return contacts_string\n",
        "\n",
        "        contacts_string = str(contacts_string).strip()\n",
        "\n",
        "        for old_name, new_chain_id in name_mapping.items():\n",
        "            contacts_string = contacts_string.replace(f\"{old_name}:\", f\"{new_chain_id}:\")\n",
        "\n",
        "        return contacts_string\n",
        "\n",
        "    def _remap_bond_chains(self, bonds_string: str, name_mapping: Dict[str, str]) -> str:\n",
        "        \"\"\"Remap covalent bond chain IDs from user names to A, B, C... format\"\"\"\n",
        "        if pd.isna(bonds_string) or str(bonds_string).strip() == '':\n",
        "            return bonds_string\n",
        "\n",
        "        bonds_string = str(bonds_string).strip()\n",
        "\n",
        "        for old_name, new_chain_id in name_mapping.items():\n",
        "            bonds_string = bonds_string.replace(f\"{old_name}:\", f\"{new_chain_id}:\")\n",
        "\n",
        "        return bonds_string\n",
        "\n",
        "    def _parse_modifications(self, mod_string: str, sequence: str, chain_id: str, seq_type: str) -> Tuple[List[Dict], List[str]]:\n",
        "        \"\"\"Parse modifications in format: CHAIN:POSITION:CCD_CODE\"\"\"\n",
        "        errors = []\n",
        "        mods = []\n",
        "\n",
        "        if pd.isna(mod_string) or str(mod_string).strip() == '':\n",
        "            return mods, errors\n",
        "\n",
        "        mod_string = str(mod_string).strip()\n",
        "        mod_entries = [e.strip() for e in mod_string.split(';') if e.strip()]\n",
        "\n",
        "        if seq_type.lower() == 'protein':\n",
        "            mod_lookups = [self.ptm_lookup, self.glycan_lookup]\n",
        "            mod_types = ['PTM', 'Glycan']\n",
        "        elif seq_type.lower() == 'dna':\n",
        "            mod_lookups = [self.dna_mod_lookup]\n",
        "            mod_types = ['DNA Mod']\n",
        "        elif seq_type.lower() == 'rna':\n",
        "            mod_lookups = [self.rna_mod_lookup]\n",
        "            mod_types = ['RNA Mod']\n",
        "        else:\n",
        "            return mods, errors\n",
        "\n",
        "        for entry in mod_entries:\n",
        "            parts = entry.split(':')\n",
        "            if len(parts) != 3:\n",
        "                errors.append(f\"Invalid modification format: '{entry}'. Use CHAIN:POSITION:CCD_CODE\")\n",
        "                continue\n",
        "\n",
        "            mod_chain, pos_str, ccd_code = parts\n",
        "            mod_chain = mod_chain.strip()\n",
        "            ccd_code = ccd_code.strip()\n",
        "\n",
        "            if mod_chain != chain_id:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                position = int(pos_str.strip())\n",
        "\n",
        "                found = False\n",
        "                for lookup, mod_type in zip(mod_lookups, mod_types):\n",
        "                    if ccd_code in lookup:\n",
        "                        found = True\n",
        "\n",
        "                        if position < 1 or position > len(sequence):\n",
        "                            errors.append(f\"{mod_type} position {position} out of range (sequence length: {len(sequence)})\")\n",
        "                            continue\n",
        "\n",
        "                        target_residue = lookup[ccd_code]['target_residue']\n",
        "                        actual_residue = sequence[position - 1].upper()\n",
        "\n",
        "                        if target_residue != 'NA':\n",
        "                            if mod_type == 'Glycan':\n",
        "                                if actual_residue not in ['N', 'T', 'S']:\n",
        "                                    errors.append(f\"Glycan {ccd_code} requires N/T/S but found {actual_residue} at position {position}\")\n",
        "                                    continue\n",
        "                            elif mod_type == 'PTM':\n",
        "                                target_1letter = self.aa_3to1.get(target_residue.upper())\n",
        "                                if target_1letter and actual_residue != target_1letter:\n",
        "                                    errors.append(f\"PTM {ccd_code} targets {target_residue}({target_1letter}) but found {actual_residue} at position {position}\")\n",
        "                                    continue\n",
        "                            else:\n",
        "                                if len(target_residue) > 1 and target_residue.startswith('D'):\n",
        "                                    target_residue = target_residue[1]\n",
        "                                if actual_residue != target_residue:\n",
        "                                    errors.append(f\"{mod_type} {ccd_code} targets {target_residue} but found {actual_residue} at position {position}\")\n",
        "                                    continue\n",
        "\n",
        "                        mods.append({\n",
        "                            'chain_id': mod_chain,\n",
        "                            'position': position,\n",
        "                            'ccd': ccd_code\n",
        "                        })\n",
        "                        break\n",
        "\n",
        "                if not found:\n",
        "                    errors.append(f\"Unknown modification code: '{ccd_code}'\")\n",
        "\n",
        "            except ValueError:\n",
        "                errors.append(f\"Invalid position in modification: '{entry}'\")\n",
        "\n",
        "        return mods, errors\n",
        "\n",
        "    def _parse_pocket(self, binder: str, contacts_string: str) -> Tuple[Optional[Dict], List[str]]:\n",
        "        \"\"\"Parse pocket constraint\"\"\"\n",
        "        errors = []\n",
        "\n",
        "        if pd.isna(binder) or str(binder).strip() == '':\n",
        "            return None, errors\n",
        "\n",
        "        binder = str(binder).strip()\n",
        "\n",
        "        if pd.isna(contacts_string) or str(contacts_string).strip() == '':\n",
        "            errors.append(f\"Pocket binder '{binder}' specified but no contacts provided\")\n",
        "            return None, errors\n",
        "\n",
        "        contacts_string = str(contacts_string).strip()\n",
        "        contact_entries = [e.strip() for e in contacts_string.split(';') if e.strip()]\n",
        "\n",
        "        contacts = []\n",
        "        for entry in contact_entries:\n",
        "            parts = entry.split(':')\n",
        "            if len(parts) != 2:\n",
        "                errors.append(f\"Invalid contact format: '{entry}'. Use CHAIN:RESIDUE\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                chain_id, residue_num = parts\n",
        "                contacts.append([chain_id.strip(), int(residue_num.strip())])\n",
        "            except ValueError:\n",
        "                errors.append(f\"Invalid contact specification: '{entry}'\")\n",
        "\n",
        "        if not contacts:\n",
        "            errors.append(f\"No valid contacts parsed for pocket binder '{binder}'\")\n",
        "            return None, errors\n",
        "\n",
        "        return {\n",
        "            'binder': binder,\n",
        "            'contacts': contacts\n",
        "        }, errors\n",
        "\n",
        "    def _parse_covalent_bonds(self, bonds_string: str) -> Tuple[List[Dict], List[str]]:\n",
        "        \"\"\"Parse covalent bond constraints\"\"\"\n",
        "        errors = []\n",
        "        bonds = []\n",
        "\n",
        "        if pd.isna(bonds_string) or str(bonds_string).strip() == '':\n",
        "            return bonds, errors\n",
        "\n",
        "        bonds_string = str(bonds_string).strip()\n",
        "        bond_entries = [e.strip() for e in bonds_string.split(';') if e.strip()]\n",
        "\n",
        "        for entry in bond_entries:\n",
        "            parts = entry.split(':')\n",
        "            if len(parts) != 6:\n",
        "                errors.append(f\"Invalid covalent bond format: '{entry}'. Use CHAIN:RES:ATOM:CHAIN:RES:ATOM\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                chain1, res1, atom1, chain2, res2, atom2 = parts\n",
        "                bonds.append({\n",
        "                    'atom1': [chain1.strip(), int(res1.strip()), atom1.strip()],\n",
        "                    'atom2': [chain2.strip(), int(res2.strip()), atom2.strip()]\n",
        "                })\n",
        "            except ValueError:\n",
        "                errors.append(f\"Invalid covalent bond specification: '{entry}'\")\n",
        "\n",
        "        return bonds, errors\n",
        "\n",
        "    def _sanitize_jobname(self, jobname: str) -> Tuple[str, List[str]]:\n",
        "        \"\"\"Sanitize jobname for filesystem compatibility\"\"\"\n",
        "        errors = []\n",
        "\n",
        "        if pd.isna(jobname) or str(jobname).strip() == '':\n",
        "            errors.append(\"Missing jobname\")\n",
        "            return '', errors\n",
        "\n",
        "        original = str(jobname)\n",
        "        sanitized = original.lower()\n",
        "        sanitized = sanitized.replace('-', '_')\n",
        "        sanitized = re.sub(r'[^a-z0-9_]', '', sanitized)\n",
        "\n",
        "        if len(sanitized) > 128:\n",
        "            sanitized = sanitized[:128]\n",
        "            errors.append(f\"Jobname truncated to 128 characters\")\n",
        "\n",
        "        if not sanitized.strip():\n",
        "            errors.append(f\"Jobname '{original}' became empty after sanitization\")\n",
        "            return '', errors\n",
        "\n",
        "        return sanitized, errors\n",
        "\n",
        "    def _generate_yaml(self, job: Dict) -> str:\n",
        "        \"\"\"Generate YAML format for Boltz-2\"\"\"\n",
        "        lines = [\"version: 1\", \"sequences:\"]\n",
        "\n",
        "        protein_groups = {}\n",
        "        dna_groups = {}\n",
        "        rna_groups = {}\n",
        "        ligand_groups = {}\n",
        "\n",
        "        for seq in job['sequences']:\n",
        "            seq_type = seq['type']\n",
        "\n",
        "            if seq_type == 'protein':\n",
        "                key = (seq['sequence'], tuple(sorted((m['position'], m['ccd']) for m in seq['modifications'])) if seq['modifications'] else ())\n",
        "                if key not in protein_groups:\n",
        "                    protein_groups[key] = []\n",
        "                protein_groups[key].append(seq)\n",
        "\n",
        "            elif seq_type == 'dna':\n",
        "                key = (seq['sequence'], tuple(sorted((m['position'], m['ccd']) for m in seq['modifications'])) if seq['modifications'] else ())\n",
        "                if key not in dna_groups:\n",
        "                    dna_groups[key] = []\n",
        "                dna_groups[key].append(seq)\n",
        "\n",
        "            elif seq_type == 'rna':\n",
        "                key = (seq['sequence'], tuple(sorted((m['position'], m['ccd']) for m in seq['modifications'])) if seq['modifications'] else ())\n",
        "                if key not in rna_groups:\n",
        "                    rna_groups[key] = []\n",
        "                rna_groups[key].append(seq)\n",
        "\n",
        "            elif seq_type == 'ligand':\n",
        "                if 'smiles' in seq:\n",
        "                    key = ('smiles', seq['smiles'])\n",
        "                else:\n",
        "                    key = ('ccd', seq['ccd'])\n",
        "                if key not in ligand_groups:\n",
        "                    ligand_groups[key] = []\n",
        "                ligand_groups[key].append(seq)\n",
        "\n",
        "        for (sequence, mod_tuple), seqs in protein_groups.items():\n",
        "            lines.append(\"  - protein:\")\n",
        "            chain_ids = [s['id'] for s in seqs]\n",
        "            if len(chain_ids) == 1:\n",
        "                lines.append(f\"      id: {chain_ids[0]}\")\n",
        "            else:\n",
        "                lines.append(f\"      id: [{', '.join(chain_ids)}]\")\n",
        "            lines.append(f\"      sequence: {sequence}\")\n",
        "\n",
        "            if seqs[0]['modifications']:\n",
        "                lines.append(\"      modifications:\")\n",
        "                for mod in seqs[0]['modifications']:\n",
        "                    lines.append(f\"        - ptmType: {mod['ccd']}\")\n",
        "                    lines.append(f\"          ptmPosition: {mod['position']}\")\n",
        "\n",
        "        for (sequence, mod_tuple), seqs in dna_groups.items():\n",
        "            lines.append(\"  - dna:\")\n",
        "            chain_ids = [s['id'] for s in seqs]\n",
        "            if len(chain_ids) == 1:\n",
        "                lines.append(f\"      id: {chain_ids[0]}\")\n",
        "            else:\n",
        "                lines.append(f\"      id: [{', '.join(chain_ids)}]\")\n",
        "            lines.append(f\"      sequence: {sequence}\")\n",
        "\n",
        "            if seqs[0]['modifications']:\n",
        "                lines.append(\"      modifications:\")\n",
        "                for mod in seqs[0]['modifications']:\n",
        "                    lines.append(f\"        - modificationType: {mod['ccd']}\")\n",
        "                    lines.append(f\"          basePosition: {mod['position']}\")\n",
        "\n",
        "        for (sequence, mod_tuple), seqs in rna_groups.items():\n",
        "            lines.append(\"  - rna:\")\n",
        "            chain_ids = [s['id'] for s in seqs]\n",
        "            if len(chain_ids) == 1:\n",
        "                lines.append(f\"      id: {chain_ids[0]}\")\n",
        "            else:\n",
        "                lines.append(f\"      id: [{', '.join(chain_ids)}]\")\n",
        "            lines.append(f\"      sequence: {sequence}\")\n",
        "\n",
        "            if seqs[0]['modifications']:\n",
        "                lines.append(\"      modifications:\")\n",
        "                for mod in seqs[0]['modifications']:\n",
        "                    lines.append(f\"        - modificationType: {mod['ccd']}\")\n",
        "                    lines.append(f\"          basePosition: {mod['position']}\")\n",
        "\n",
        "        for (lig_type, lig_value), seqs in ligand_groups.items():\n",
        "            lines.append(\"  - ligand:\")\n",
        "            chain_ids = [s['id'] for s in seqs]\n",
        "            if len(chain_ids) == 1:\n",
        "                lines.append(f\"      id: {chain_ids[0]}\")\n",
        "            else:\n",
        "                lines.append(f\"      id: [{', '.join(chain_ids)}]\")\n",
        "\n",
        "            if lig_type == 'smiles':\n",
        "                lines.append(f\"      smiles: '{lig_value}'\")\n",
        "            else:\n",
        "                lines.append(f\"      ccd: {lig_value}\")\n",
        "\n",
        "        if job.get('pocket') or job.get('covalent_bonds'):\n",
        "            lines.append(\"constraints:\")\n",
        "\n",
        "            if job.get('pocket'):\n",
        "                pocket = job['pocket']\n",
        "                lines.append(\"  - pocket:\")\n",
        "                lines.append(f\"      binder: {pocket['binder']}\")\n",
        "                lines.append(\"      contacts:\")\n",
        "                for contact in pocket['contacts']:\n",
        "                    lines.append(f\"        - [{contact[0]}, {contact[1]}]\")\n",
        "\n",
        "            if job.get('covalent_bonds'):\n",
        "                for bond in job['covalent_bonds']:\n",
        "                    lines.append(\"  - bond:\")\n",
        "                    lines.append(f\"      atom1: [{bond['atom1'][0]}, {bond['atom1'][1]}, {bond['atom1'][2]}]\")\n",
        "                    lines.append(f\"      atom2: [{bond['atom2'][0]}, {bond['atom2'][1]}, {bond['atom2'][2]}]\")\n",
        "\n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "    def _process_job(self, row: pd.Series) -> Tuple[Optional[Dict], List[str]]:\n",
        "        \"\"\"Process a single job row from CSV\"\"\"\n",
        "        errors = []\n",
        "        all_sequences = []\n",
        "\n",
        "        jobname, jobname_errors = self._sanitize_jobname(row.get('jobname', ''))\n",
        "        errors.extend(jobname_errors)\n",
        "\n",
        "        if not jobname:\n",
        "            return None, errors\n",
        "\n",
        "        pocket_binder = row.get('pocket_binder', '')\n",
        "        pocket_contacts = row.get('pocket_contacts', '')\n",
        "        covalent_bonds_str = row.get('covalent_bonds', '')\n",
        "\n",
        "        chain_id_counter = 0\n",
        "        name_to_chain_mapping = {}\n",
        "\n",
        "        for i in range(1, 11):\n",
        "            name_col = f'seq{i}_name'\n",
        "            type_col = f'seq{i}_type'\n",
        "            copies_col = f'seq{i}_copies'\n",
        "            seq_col = f'seq{i}'\n",
        "            mods_col = f'seq{i}_mods'\n",
        "\n",
        "            if name_col not in row or type_col not in row or seq_col not in row:\n",
        "                continue\n",
        "\n",
        "            user_name = row.get(name_col, '')\n",
        "            seq_type = row.get(type_col, '')\n",
        "            copies = row.get(copies_col, 1)\n",
        "            sequence = row.get(seq_col, '')\n",
        "            mods = row.get(mods_col, '')\n",
        "\n",
        "            if pd.isna(sequence) or str(sequence).strip() == '':\n",
        "                continue\n",
        "\n",
        "            sequence = str(sequence).strip()\n",
        "            copies = int(copies) if pd.notna(copies) and str(copies).strip() != '' else 1\n",
        "            user_name = str(user_name).strip() if pd.notna(user_name) else ''\n",
        "\n",
        "            chain_ids = []\n",
        "            for copy_num in range(copies):\n",
        "                chain_id = chr(ord('A') + chain_id_counter)\n",
        "                chain_ids.append(chain_id)\n",
        "\n",
        "                if user_name and copy_num == 0:\n",
        "                    name_to_chain_mapping[user_name] = chain_id\n",
        "\n",
        "                chain_id_counter += 1\n",
        "\n",
        "            if seq_type.lower() in ['protein', 'dna', 'rna']:\n",
        "                char_errors = self._validate_sequence_characters(sequence, seq_type)\n",
        "                errors.extend(char_errors)\n",
        "\n",
        "                for idx, chain_id in enumerate(chain_ids):\n",
        "                    remapped_mods = self._remap_modification_chains(mods, name_to_chain_mapping)\n",
        "                    mods_list, mod_errors = self._parse_modifications(remapped_mods, sequence, chain_id, seq_type)\n",
        "                    errors.extend(mod_errors)\n",
        "\n",
        "                    seq_dict = {\n",
        "                        'type': seq_type.lower(),\n",
        "                        'id': chain_id,\n",
        "                        'sequence': sequence,\n",
        "                        'modifications': mods_list if mods_list else None\n",
        "                    }\n",
        "                    all_sequences.append(seq_dict)\n",
        "\n",
        "            elif seq_type.lower() == 'ligand':\n",
        "                ligand_string = sequence.strip()\n",
        "                is_smiles = self._is_smiles(ligand_string)\n",
        "\n",
        "                if not is_smiles:\n",
        "                    if ligand_string not in self.ligand_lookup and ligand_string not in self.ion_lookup:\n",
        "                        errors.append(f\"Unknown ligand/ion CCD code: '{ligand_string}'\")\n",
        "\n",
        "                for chain_id in chain_ids:\n",
        "                    if is_smiles:\n",
        "                        seq_dict = {\n",
        "                            'type': 'ligand',\n",
        "                            'id': chain_id,\n",
        "                            'smiles': ligand_string\n",
        "                        }\n",
        "                    else:\n",
        "                        seq_dict = {\n",
        "                            'type': 'ligand',\n",
        "                            'id': chain_id,\n",
        "                            'ccd': ligand_string\n",
        "                        }\n",
        "                    all_sequences.append(seq_dict)\n",
        "            else:\n",
        "                errors.append(f\"Unsupported sequence type: {seq_type}\")\n",
        "\n",
        "        if not all_sequences:\n",
        "            errors.append(\"No valid sequences found\")\n",
        "            return None, errors\n",
        "\n",
        "        remapped_pocket_binder = name_to_chain_mapping.get(pocket_binder, pocket_binder) if pocket_binder else pocket_binder\n",
        "        remapped_contacts = self._remap_contact_chains(pocket_contacts, name_to_chain_mapping)\n",
        "\n",
        "        pocket, pocket_errors = self._parse_pocket(remapped_pocket_binder, remapped_contacts)\n",
        "        errors.extend(pocket_errors)\n",
        "\n",
        "        remapped_bonds = self._remap_bond_chains(covalent_bonds_str, name_to_chain_mapping)\n",
        "        covalent_bonds, bond_errors = self._parse_covalent_bonds(remapped_bonds)\n",
        "        errors.extend(bond_errors)\n",
        "\n",
        "        has_modifications = any(seq.get('modifications') for seq in all_sequences)\n",
        "\n",
        "        job = {\n",
        "            'name': jobname,\n",
        "            'sequences': all_sequences,\n",
        "            'pocket': pocket,\n",
        "            'covalent_bonds': covalent_bonds,\n",
        "            'has_modifications': has_modifications,\n",
        "            'has_pocket': pocket is not None,\n",
        "            'has_covalent': len(covalent_bonds) > 0\n",
        "        }\n",
        "\n",
        "        return job, errors\n",
        "\n",
        "    def process_csv(self, csv_path: str) -> Tuple[List[Dict], pd.DataFrame]:\n",
        "        \"\"\"Process CSV file and return jobs list and summary DataFrame\"\"\"\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        jobs = []\n",
        "        summary_rows = []\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            job, errors = self._process_job(row)\n",
        "\n",
        "            if job:\n",
        "                jobs.append(job)\n",
        "                summary_rows.append({\n",
        "                    'jobname': job['name'],\n",
        "                    'sequences': len(job['sequences']),\n",
        "                    'has_modifications': job['has_modifications'],\n",
        "                    'has_pocket': job['has_pocket'],\n",
        "                    'has_covalent': job['has_covalent'],\n",
        "                    'status': 'ERROR' if errors else 'OK',\n",
        "                    'errors': '; '.join(errors) if errors else ''\n",
        "                })\n",
        "            else:\n",
        "                summary_rows.append({\n",
        "                    'jobname': f\"Row {idx+1}\",\n",
        "                    'sequences': 0,\n",
        "                    'has_modifications': False,\n",
        "                    'has_pocket': False,\n",
        "                    'has_covalent': False,\n",
        "                    'status': 'FAILED',\n",
        "                    'errors': '; '.join(errors)\n",
        "                })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_rows)\n",
        "        return jobs, summary_df\n",
        "\n",
        "print(\"üîß Initializing Boltz CSV Processor...\")\n",
        "boltz_processor = BoltzJobProcessor()\n",
        "\n",
        "print(\"‚úÖ Using embedded reference data: 79 entries\")\n",
        "print(\"‚úÖ Processor ready\")\n",
        "print(\"üìã Reference data includes:\")\n",
        "print(f\"   ‚Ä¢ 15 PTM types\")\n",
        "print(f\"   ‚Ä¢ 24 ligand types\")\n",
        "print(f\"   ‚Ä¢ 11 ion types\")\n",
        "print(f\"   ‚Ä¢ 10 glycan types\")\n",
        "print(f\"   ‚Ä¢ 8 DNA modification types\")\n",
        "print(f\"   ‚Ä¢ 10 RNA modification types\")\n",
        "print(\"\\nüí° Using embedded reference data (common PTMs, ligands, ions, glycans, DNA/RNA mods)\")\n",
        "print(\"   To use custom reference: upload file in Cell 3\")\n",
        "print(\"\\nüìù Note: Chain IDs are assigned as A, B, C, D... sequentially\")\n",
        "print(\"   Sequence identity is preserved in job names\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WcbddZPH52WA",
        "outputId": "1163297b-20eb-4458-9a9f-ec40ba507972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üìÅ CSV UPLOAD FOR BATCH PROCESSING\n",
            "============================================================\n",
            "\n",
            "üìä Upload your input CSV file with job specifications\n",
            "Required columns: jobname, seq1_name, seq1_type, seq1\n",
            "Optional: seq1_copies, seq1_mods, pocket_binder, pocket_contacts, covalent_bonds\n",
            "Supports up to 10 sequences per job (seq1 through seq10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f90c3d0b-047f-47d7-9307-5124a08ee5ef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f90c3d0b-047f-47d7-9307-5124a08ee5ef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving boltz_csv_template_Nick.csv to boltz_csv_template_Nick.csv\n",
            "\n",
            "‚úÖ Uploaded: boltz_csv_template_Nick.csv\n",
            "\n",
            "üîÑ Processing CSV...\n",
            "\n",
            "============================================================\n",
            "üìã JOB SUMMARY\n",
            "============================================================\n",
            "                jobname  sequences  has_modifications  has_pocket  has_covalent status errors\n",
            "     hrip1_xp_016502092          2              False       False         False     OK       \n",
            "     hrip1_xp_016467230          2              False       False         False     OK       \n",
            "     hrip1_xp_016454296          2              False       False         False     OK       \n",
            "     hrip1_xp_016481883          2              False       False         False     OK       \n",
            "   mohrip1_xp_016448570          2              False       False         False     OK       \n",
            "   mohrip1_xp_016436137          2              False       False         False     OK       \n",
            "     mgnlp_xp_016504433          2              False       False         False     OK       \n",
            "     mgnlp_xp_016475599          2              False       False         False     OK       \n",
            "  mgnlp_xp_xp_016454296          2              False       False         False     OK       \n",
            "     mgnlp_xp_016462444          2              False       False         False     OK       \n",
            "     mgnlp_xp_016512503          2              False       False         False     OK       \n",
            "     mgnlp_xp_016436748          2              False       False         False     OK       \n",
            "     mgnlp_xp_016475135          2              False       False         False     OK       \n",
            "    rccdi1_xp_016433134          2              False       False         False     OK       \n",
            "      zt11_xp_016453375          2              False       False         False     OK       \n",
            "  dsecp2_1_xp_016445294          2              False       False         False     OK       \n",
            "  dsecp2_1_xp_016479726          2              False       False         False     OK       \n",
            "     ssne4_xp_016467168          2              False       False         False     OK       \n",
            "       zt5_xp_016476064          2              False       False         False     OK       \n",
            "    grpel2_xp_016446164          2              False       False         False     OK       \n",
            "   grexpb2_xp_016467168          2              False       False         False     OK       \n",
            "pitg_23123_xp_016440461          2              False       False         False     OK       \n",
            "    bccdi1_xp_016456877          2              False       False         False     OK       \n",
            "     vdeg1_xp_016462662          2              False       False         False     OK       \n",
            "     ssne5_xp_016509707          2              False       False         False     OK       \n",
            "     ssne5_xp_016503974          2              False       False         False     OK       \n",
            "     vdeg3_xp_016499809          2              False       False         False     OK       \n",
            "     vdeg3_xp_016476018          2              False       False         False     OK       \n",
            "   ds70057_xp_016499270          2              False       False         False     OK       \n",
            "    bccfem_xp_016456949          2              False       False         False     OK       \n",
            "pitg_08055_xp_016453348          2              False       False         False     OK       \n",
            "     ssne1_xp_016498905          2              False       False         False     OK       \n",
            "    bcxyi1_xp_016476778          2              False       False         False     OK       \n",
            "   cs02526_xp_016456877          2              False       False         False     OK       \n",
            "      fg62_xp_016511217          2              False       False         False     OK       \n",
            "   pd_0956_xp_016494700          2              False       False         False     OK       \n",
            "      zt13_xp_016516008          2              False       False         False     OK       \n",
            "\n",
            "‚òÅÔ∏è  Setting up Google Drive...\n",
            "‚úÖ Google Drive connected\n",
            "\n",
            "============================================================\n",
            "‚úÖ READY TO PROCESS 37 JOBS\n",
            "============================================================\n",
            "\n",
            "üìå Next steps:\n",
            "  1. Configure MSA settings (Cell 4)\n",
            "  2. Configure prediction parameters (Cell 5)\n",
            "  3. Run batch predictions (Cell 6)\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 3: Upload CSV and Configure Batch Jobs\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration options\n",
        "upload_custom_reference = False #@param {type:\"boolean\"}\n",
        "#@markdown - Upload custom reference file (optional - embedded data includes common PTMs/ligands)\n",
        "\n",
        "setup_google_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown - Setup Google Drive for automatic result upload\n",
        "\n",
        "gdrive_folder_name = \"Boltz2_Predictions\" #@param {type:\"string\"}\n",
        "#@markdown - Google Drive folder name for batch results\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìÅ CSV UPLOAD FOR BATCH PROCESSING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Handle custom reference file upload if requested\n",
        "custom_ref_file = None\n",
        "if upload_custom_reference:\n",
        "    print(\"\\nüì§ Upload custom reference CSV file...\")\n",
        "    print(\"Required columns: Type, CCD Code, Name, Target Residue, Heavy Atom Count\")\n",
        "    uploaded_ref = files.upload()\n",
        "\n",
        "    if uploaded_ref:\n",
        "        custom_ref_file = list(uploaded_ref.keys())[0]\n",
        "        print(f\"‚úÖ Custom reference uploaded: {custom_ref_file}\")\n",
        "\n",
        "# Initialize processor (will use embedded data if no custom file)\n",
        "try:\n",
        "    boltz_processor = BoltzJobProcessor(custom_ref_file)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize processor: {e}\")\n",
        "    raise\n",
        "\n",
        "# Upload input CSV\n",
        "print(\"\\nüìä Upload your input CSV file with job specifications\")\n",
        "print(\"Required columns: jobname, seq1_name, seq1_type, seq1\")\n",
        "print(\"Optional: seq1_copies, seq1_mods, pocket_binder, pocket_contacts, covalent_bonds\")\n",
        "print(\"Supports up to 10 sequences per job (seq1 through seq10)\")\n",
        "\n",
        "uploaded_csv = files.upload()\n",
        "\n",
        "if not uploaded_csv:\n",
        "    raise ValueError(\"No CSV file uploaded\")\n",
        "\n",
        "csv_filename = list(uploaded_csv.keys())[0]\n",
        "print(f\"\\n‚úÖ Uploaded: {csv_filename}\")\n",
        "\n",
        "# Process CSV\n",
        "print(\"\\nüîÑ Processing CSV...\")\n",
        "try:\n",
        "    jobs, summary_df = boltz_processor.process_csv(csv_filename)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå CSV processing failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìã JOB SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Check for errors\n",
        "error_jobs = summary_df[summary_df['status'] == 'ERROR']\n",
        "if len(error_jobs) > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  {len(error_jobs)} jobs have errors:\")\n",
        "    for _, row in error_jobs.iterrows():\n",
        "        print(f\"  ‚Ä¢ {row['jobname']}: {row['errors']}\")\n",
        "\n",
        "    proceed = input(\"\\nProceed with valid jobs only? (yes/no): \").strip().lower()\n",
        "    if proceed not in ['yes', 'y']:\n",
        "        raise ValueError(\"Processing cancelled by user\")\n",
        "\n",
        "# Setup Google Drive if requested\n",
        "drive = None\n",
        "if setup_google_drive:\n",
        "    try:\n",
        "        from pydrive2.drive import GoogleDrive\n",
        "        from pydrive2.auth import GoogleAuth\n",
        "        from google.colab import auth\n",
        "        from oauth2client.client import GoogleCredentials\n",
        "\n",
        "        print(\"\\n‚òÅÔ∏è  Setting up Google Drive...\")\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "        print(\"‚úÖ Google Drive connected\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Google Drive setup failed: {e}\")\n",
        "        drive = None\n",
        "\n",
        "# Store in global settings\n",
        "if 'global_settings' not in globals():\n",
        "    global_settings = {}\n",
        "\n",
        "global_settings.update({\n",
        "    'batch_jobs': jobs,\n",
        "    'csv_filename': csv_filename,\n",
        "    'drive': drive,\n",
        "    'gdrive_folder_name': gdrive_folder_name,\n",
        "    'processor': boltz_processor\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"‚úÖ READY TO PROCESS {len(jobs)} JOBS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüìå Next steps:\")\n",
        "print(\"  1. Configure MSA settings (Cell 4)\")\n",
        "print(\"  2. Configure prediction parameters (Cell 5)\")\n",
        "print(\"  3. Run batch predictions (Cell 6)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnrh_B6Y92ph",
        "outputId": "0c4a5320-8e6d-4eaa-d3d6-963d4f65647a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ MSA configuration set:\n",
            "  Mode: mmseqs2_uniref_env\n",
            "  Pairing strategy: greedy\n",
            "  Use MSA server: True\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 4: MSA Configuration\n",
        "msa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\", \"single_sequence\"]\n",
        "#@markdown - **MSA generation method** (mmseqs2 modes use ColabFold server)\n",
        "\n",
        "msa_pairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"]\n",
        "#@markdown - **Pairing strategy**: `greedy` = pair any matching subsets, `complete` = all sequences must match\n",
        "\n",
        "# Check if global_settings exists\n",
        "if 'global_settings' not in globals():\n",
        "    print(\"‚ö†Ô∏è  Please run the CSV Upload cell first\")\n",
        "else:\n",
        "    # Configure MSA settings\n",
        "    if \"mmseqs2\" in msa_mode:\n",
        "        use_msa_server = True\n",
        "        msa_server_url = \"https://api.colabfold.com\"\n",
        "    else:\n",
        "        use_msa_server = False\n",
        "        msa_server_url = None\n",
        "\n",
        "    # Store MSA settings\n",
        "    global_settings.update({\n",
        "        'msa_mode': msa_mode,\n",
        "        'msa_pairing_strategy': msa_pairing_strategy,\n",
        "        'use_msa_server': use_msa_server,\n",
        "        'msa_server_url': msa_server_url\n",
        "    })\n",
        "\n",
        "    print(\"‚úÖ MSA configuration set:\")\n",
        "    print(f\"  Mode: {msa_mode}\")\n",
        "    print(f\"  Pairing strategy: {msa_pairing_strategy}\")\n",
        "    print(f\"  Use MSA server: {use_msa_server}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3am6YdJ92zA",
        "outputId": "dc128371-effa-4d89-b623-bf0dc6b58ec0",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Advanced settings configured:\n",
            "  Recycling steps: 6\n",
            "  Sampling steps: 200\n",
            "  Diffusion samples: 5\n",
            "  Predict affinity: False\n",
            "  Output format: mmcif\n",
            "  Use potentials: True\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 5: Advanced Prediction Settings\n",
        "# Structure Prediction Settings\n",
        "recycling_steps = 6 #@param {type:\"integer\"}\n",
        "#@markdown - **Iterative refinement passes**: Each cycle refines the structure using updated predictions. Higher values improve local geometry and confidence scores. **Time**: ~linear scaling (3 steps = 3x base time). **VRAM**: +20-30% per additional step for intermediate states.\n",
        "\n",
        "sampling_steps = 200 #@param {type:\"integer\"}\n",
        "#@markdown - **Diffusion denoising iterations**: Controls how many steps the diffusion model takes to generate structures from noise. More steps = smoother, higher quality structures. **Time**: Linear scaling (50 steps = 4x faster than 200). **VRAM**: +10-15% for intermediate diffusion states.\n",
        "\n",
        "diffusion_samples = 5 #@param {type:\"integer\"}\n",
        "#@markdown - **Independent structure predictions**: Number of different structures generated per input. More samples increase diversity and reliability of results. **Time**: Linear scaling (5 samples = 5x base time). **VRAM**: Depends on max_parallel_samples setting.\n",
        "\n",
        "max_parallel_samples = 5 #@param {type:\"integer\"}\n",
        "#@markdown - **GPU memory management**: How many diffusion samples are processed simultaneously. Critical for large complexes - each parallel sample requires full model memory allocation. **Time**: Minimal impact on total time. **VRAM**: ~Linear scaling (2 parallel = ~2x memory, 5 parallel = ~5x memory).\n",
        "\n",
        "step_scale = 1.638 #@param {type:\"number\"}\n",
        "#@markdown - **Sampling temperature**: Controls randomness in structure generation. Higher values increase diversity but may reduce quality. 1.638 is optimized default. **Time**: No impact. **VRAM**: No impact.\n",
        "\n",
        "# Affinity Prediction Settings\n",
        "predict_affinity = False #@param {type:\"boolean\"}\n",
        "#@markdown - **Binding strength prediction**: Runs additional affinity model to predict binding strength (Kd/Ki values). Most reliable for protein-small molecule complexes. **Time**: +50-100% total time. **VRAM**: +40-60% for affinity model loading.\n",
        "\n",
        "affinity_mw_correction = False #@param {type:\"boolean\"}\n",
        "#@markdown - **Molecular weight adjustment**: Applies size-based corrections to affinity predictions. Only affects affinity calculation, not structure. **Time**: Minimal impact. **VRAM**: No impact.\n",
        "\n",
        "sampling_steps_affinity = 200 #@param {type:\"integer\"}\n",
        "#@markdown - **Affinity model diffusion steps**: Controls quality of affinity predictions. Similar to sampling_steps but for the affinity model. **Time**: Linear scaling within affinity prediction. **VRAM**: +5-10% for affinity diffusion states.\n",
        "\n",
        "diffusion_samples_affinity = 5 #@param {type:\"integer\"}\n",
        "#@markdown - **Affinity prediction ensemble size**: Number of independent affinity predictions to average for final binding strength. More samples = more reliable Kd estimates. **Time**: Linear scaling for affinity portion. **VRAM**: Minimal additional impact.\n",
        "\n",
        "# Output and Optimization Settings\n",
        "output_format = \"mmcif\" #@param [\"mmcif\", \"pdb\"]\n",
        "#@markdown - **Structure file format**: mmCIF supports more metadata and modern features, PDB is more widely compatible. Both contain same structural information. **Time**: No impact. **VRAM**: No impact.\n",
        "\n",
        "write_full_pae = True #@param {type:\"boolean\"}\n",
        "#@markdown - **Save Predicted Aligned Error matrix**: Confidence scores between all residue pairs. Essential for assessing interface quality and domain reliability. **Time**: +5-10% for matrix computation and I/O. **VRAM**: +10-20% for large complexes during matrix storage.\n",
        "\n",
        "write_full_pde = False #@param {type:\"boolean\"}\n",
        "#@markdown - **Save Predicted Distance Error matrix**: Distance confidence predictions between residue pairs. Useful for validation and uncertainty quantification. **Time**: +5-10% for matrix computation and I/O. **VRAM**: +10-20% for large complexes during matrix storage.\n",
        "\n",
        "use_potentials = True #@param {type:\"boolean\"}\n",
        "#@markdown - **Inference-time physics optimization**: Applies physics-based energy minimization to improve local geometry and remove clashes. Significantly improves structure quality, especially for interfaces. **Time**: +30-50% total time. **VRAM**: +15-25% for physics calculation buffers.\n",
        "\n",
        "# Check if global_settings exists\n",
        "if 'global_settings' not in globals():\n",
        "    print(\"‚ö†Ô∏è  Please run the 'Choose Input Method' cell first\")\n",
        "else:\n",
        "    # Store advanced settings\n",
        "    advanced_settings = {\n",
        "        'recycling_steps': recycling_steps,\n",
        "        'sampling_steps': sampling_steps,\n",
        "        'diffusion_samples': diffusion_samples,\n",
        "        'max_parallel_samples': max_parallel_samples,\n",
        "        'step_scale': step_scale,\n",
        "        'predict_affinity': predict_affinity,\n",
        "        'affinity_mw_correction': affinity_mw_correction,\n",
        "        'sampling_steps_affinity': sampling_steps_affinity,\n",
        "        'diffusion_samples_affinity': diffusion_samples_affinity,\n",
        "        'output_format': output_format,\n",
        "        'write_full_pae': write_full_pae,\n",
        "        'write_full_pde': write_full_pde,\n",
        "        'use_potentials': use_potentials,\n",
        "        'max_msa_seqs': 8192,\n",
        "        'subsample_msa': False,\n",
        "        'num_subsampled_msa': 1024\n",
        "    }\n",
        "\n",
        "    global_settings.update(advanced_settings)\n",
        "\n",
        "    print(\"‚úÖ Advanced settings configured:\")\n",
        "    print(f\"  Recycling steps: {recycling_steps}\")\n",
        "    print(f\"  Sampling steps: {sampling_steps}\")\n",
        "    print(f\"  Diffusion samples: {diffusion_samples}\")\n",
        "    print(f\"  Predict affinity: {predict_affinity}\")\n",
        "    print(f\"  Output format: {output_format}\")\n",
        "    print(f\"  Use potentials: {use_potentials}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfaAZLH1-6M6",
        "outputId": "a85703be-3339-421c-b2d3-af674f3b5bfc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Checking GPU availability...\n",
            "‚úÖ GPU: NVIDIA A100-SXM4-40GB (42.5 GB)\n",
            "\n",
            "üîß Kernel mode: --no_kernels\n",
            "   (Using CPU fallback - slower but more compatible)\n",
            "‚úÖ Found existing folder: Boltz2_Predictions\n",
            "\n",
            "============================================================\n",
            "üöÄ STARTING BATCH PROCESSING\n",
            "============================================================\n",
            "Total jobs: 37\n",
            "Started: 2025-10-27 16:11:58\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "üöÄ Job 1/37: hrip1_xp_016502092\n",
            "============================================================\n",
            "üìù Generated YAML configuration\n",
            "üîß Command: boltz predict hrip1_xp_016502092/hrip1_xp_016502092.yaml --out_dir hrip1_xp_016502092 --recycling_steps 6 --sampling_steps 200 --diffusion_samples 5 --max_parallel_samples 5 --step_scale 1.638 --output_format mmcif --max_msa_seqs 8192 --override --no_kernels --use_msa_server --msa_server_url https://api.colabfold.com --msa_pairing_strategy greedy --write_full_pae\n",
            "\n",
            "üìã Boltz output/warnings:\n",
            "   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "     0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   SUBMIT:   0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   COMPLETE:   0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   COMPLETE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [elapsed: 00:00 remaining: 00:00]\n",
            "   COMPLETE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [elapsed: 00:01 remaining: 00:00]\n",
            "     0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   SUBMIT:   0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   COMPLETE:   0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   COMPLETE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [elapsed: 00:00 remaining: 00:00]\n",
            "   COMPLETE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [elapsed: 00:07 remaining: 00:00]\n",
            "   100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:11<00:00, 11.50s/it]\n",
            "   100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:11<00:00, 11.50s/it]\n",
            "   Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "   GPU available: True (cuda), used: True\n",
            "   TPU available: False, using: 0 TPU cores\n",
            "   HPU available: False, using: 0 HPUs\n",
            "   /usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.5.0.post0, which is newer than your current Lightning version: v2.5.0\n",
            "   You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "   2025-10-27 16:13:21.008076: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "   2025-10-27 16:13:21.024071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "   WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "   E0000 00:00:1761581601.042025    5034 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "   E0000 00:00:1761581601.047449    5034 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "   W0000 00:00:1761581601.061204    5034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   W0000 00:00:1761581601.061223    5034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   W0000 00:00:1761581601.061225    5034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   W0000 00:00:1761581601.061227    5034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   2025-10-27 16:13:21.065188: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "   To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "   LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "‚úÖ Generated 5 structure files\n",
            "   üìÑ hrip1_xp_016502092_model_0.cif\n",
            "   üìÑ hrip1_xp_016502092_model_4.cif\n",
            "   üìÑ hrip1_xp_016502092_model_3.cif\n",
            "   üìÑ hrip1_xp_016502092_model_1.cif\n",
            "   üìÑ hrip1_xp_016502092_model_2.cif\n",
            "üì¶ Created: hrip1_xp_016502092_results.zip\n",
            "  ‚òÅÔ∏è  Uploaded to Google Drive: https://drive.google.com/file/d/1QcmQFHlveABCiDnZEP6qogjJ46aR78gH/view\n",
            "‚è±Ô∏è  Completed in 383.4s\n",
            "\n",
            "============================================================\n",
            "üöÄ Job 2/37: hrip1_xp_016467230\n",
            "============================================================\n",
            "üìù Generated YAML configuration\n",
            "üîß Command: boltz predict hrip1_xp_016467230/hrip1_xp_016467230.yaml --out_dir hrip1_xp_016467230 --recycling_steps 6 --sampling_steps 200 --diffusion_samples 5 --max_parallel_samples 5 --step_scale 1.638 --output_format mmcif --max_msa_seqs 8192 --override --no_kernels --use_msa_server --msa_server_url https://api.colabfold.com --msa_pairing_strategy greedy --write_full_pae\n",
            "\n",
            "üìã Boltz output/warnings:\n",
            "   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "     0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   SUBMIT:   0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   COMPLETE:   0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   COMPLETE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [elapsed: 00:00 remaining: 00:00]\n",
            "   COMPLETE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [elapsed: 00:01 remaining: 00:00]\n",
            "     0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   SUBMIT:   0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   COMPLETE:   0%|          | 0/300 [elapsed: 00:00 remaining: ?]\n",
            "   COMPLETE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [elapsed: 00:00 remaining: 00:00]\n",
            "   COMPLETE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [elapsed: 00:05 remaining: 00:00]\n",
            "   100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:09<00:00,  9.60s/it]\n",
            "   100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:09<00:00,  9.60s/it]\n",
            "   Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "   GPU available: True (cuda), used: True\n",
            "   TPU available: False, using: 0 TPU cores\n",
            "   HPU available: False, using: 0 HPUs\n",
            "   /usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.5.0.post0, which is newer than your current Lightning version: v2.5.0\n",
            "   You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "   2025-10-27 16:19:07.790337: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "   2025-10-27 16:19:07.807046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "   WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "   E0000 00:00:1761581947.825392    6639 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "   E0000 00:00:1761581947.830846    6639 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "   W0000 00:00:1761581947.845302    6639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   W0000 00:00:1761581947.845320    6639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   W0000 00:00:1761581947.845323    6639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   W0000 00:00:1761581947.845324    6639 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   2025-10-27 16:19:07.849360: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "   To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "   LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "‚úÖ Generated 5 structure files\n",
            "   üìÑ hrip1_xp_016467230_model_0.cif\n",
            "   üìÑ hrip1_xp_016467230_model_1.cif\n",
            "   üìÑ hrip1_xp_016467230_model_3.cif\n",
            "   üìÑ hrip1_xp_016467230_model_2.cif\n",
            "   üìÑ hrip1_xp_016467230_model_4.cif\n",
            "üì¶ Created: hrip1_xp_016467230_results.zip\n",
            "  ‚òÅÔ∏è  Uploaded to Google Drive: https://drive.google.com/file/d/134MULy3EkxiTd1QesdHCt5allT59hAwl/view\n",
            "‚è±Ô∏è  Completed in 349.8s\n",
            "\n",
            "============================================================\n",
            "üöÄ Job 3/37: hrip1_xp_016454296\n",
            "============================================================\n",
            "üìù Generated YAML configuration\n",
            "üîß Command: boltz predict hrip1_xp_016454296/hrip1_xp_016454296.yaml --out_dir hrip1_xp_016454296 --recycling_steps 6 --sampling_steps 200 --diffusion_samples 5 --max_parallel_samples 5 --step_scale 1.638 --output_format mmcif --max_msa_seqs 8192 --override --no_kernels --use_msa_server --msa_server_url https://api.colabfold.com --msa_pairing_strategy greedy --write_full_pae\n",
            "\n",
            "üìã Boltz output/warnings:\n",
            "   RUNNING:   8%|‚ñä         | 23/300 [elapsed: 00:35 remaining: 05:06]\n",
            "   RUNNING:  11%|‚ñà         | 32/300 [elapsed: 00:35 remaining: 04:53]Sleeping for 10s. Reason: RUNNING\n",
            "   RUNNING:  11%|‚ñà         | 32/300 [elapsed: 00:46 remaining: 04:53]\n",
            "   RUNNING:  14%|‚ñà‚ñç        | 42/300 [elapsed: 00:46 remaining: 04:40]Sleeping for 10s. Reason: RUNNING\n",
            "   RUNNING:  14%|‚ñà‚ñç        | 42/300 [elapsed: 00:57 remaining: 04:40]\n",
            "   RUNNING:  17%|‚ñà‚ñã        | 52/300 [elapsed: 00:57 remaining: 04:28]Sleeping for 7s. Reason: RUNNING\n",
            "   RUNNING:  17%|‚ñà‚ñã        | 52/300 [elapsed: 01:04 remaining: 04:28]\n",
            "   RUNNING:  20%|‚ñà‚ñâ        | 59/300 [elapsed: 01:04 remaining: 04:21]Sleeping for 8s. Reason: RUNNING\n",
            "   RUNNING:  20%|‚ñà‚ñâ        | 59/300 [elapsed: 01:13 remaining: 04:21]\n",
            "   RUNNING:  22%|‚ñà‚ñà‚ñè       | 67/300 [elapsed: 01:13 remaining: 04:13]Sleeping for 10s. Reason: RUNNING\n",
            "   COMPLETE:  22%|‚ñà‚ñà‚ñè       | 67/300 [elapsed: 01:24 remaining: 04:13]\n",
            "   COMPLETE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [elapsed: 01:24 remaining: 00:00]\n",
            "   COMPLETE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [elapsed: 01:28 remaining: 00:00]\n",
            "   100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [02:07<00:00, 127.27s/it]\n",
            "   100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [02:07<00:00, 127.27s/it]\n",
            "   Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "   GPU available: True (cuda), used: True\n",
            "   TPU available: False, using: 0 TPU cores\n",
            "   HPU available: False, using: 0 HPUs\n",
            "   /usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.5.0.post0, which is newer than your current Lightning version: v2.5.0\n",
            "   You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "   2025-10-27 16:26:55.111416: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "   2025-10-27 16:26:55.127151: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "   WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "   E0000 00:00:1761582415.145081    8112 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "   E0000 00:00:1761582415.150402    8112 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "   W0000 00:00:1761582415.164079    8112 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   W0000 00:00:1761582415.164098    8112 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   W0000 00:00:1761582415.164100    8112 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   W0000 00:00:1761582415.164102    8112 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "   2025-10-27 16:26:55.168130: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "   To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "   LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "‚úÖ Generated 5 structure files\n",
            "   üìÑ hrip1_xp_016454296_model_3.cif\n",
            "   üìÑ hrip1_xp_016454296_model_2.cif\n",
            "   üìÑ hrip1_xp_016454296_model_4.cif\n",
            "   üìÑ hrip1_xp_016454296_model_1.cif\n",
            "   üìÑ hrip1_xp_016454296_model_0.cif\n",
            "üì¶ Created: hrip1_xp_016454296_results.zip\n",
            "  ‚òÅÔ∏è  Uploaded to Google Drive: https://drive.google.com/file/d/14jf7XjdSrMgu20krndlUnwTYO_cFuS6w/view\n",
            "‚è±Ô∏è  Completed in 339.6s\n",
            "\n",
            "============================================================\n",
            "üöÄ Job 4/37: hrip1_xp_016481883\n",
            "============================================================\n",
            "üìù Generated YAML configuration\n",
            "üîß Command: boltz predict hrip1_xp_016481883/hrip1_xp_016481883.yaml --out_dir hrip1_xp_016481883 --recycling_steps 6 --sampling_steps 200 --diffusion_samples 5 --max_parallel_samples 5 --step_scale 1.638 --output_format mmcif --max_msa_seqs 8192 --override --no_kernels --use_msa_server --msa_server_url https://api.colabfold.com --msa_pairing_strategy greedy --write_full_pae\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 6: Run Batch Predictions with Smart Kernel Detection\n",
        "%%time\n",
        "import subprocess\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Verify setup\n",
        "if 'global_settings' not in globals():\n",
        "    print(\"‚ùå Error: Please run the previous configuration cells first\")\n",
        "elif not global_settings.get('batch_jobs'):\n",
        "    print(\"‚ùå Error: No batch jobs configured. Run CSV upload cell first\")\n",
        "else:\n",
        "    # GPU verification\n",
        "    print(\"üîç Checking GPU availability...\")\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB)\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  WARNING: No GPU detected - predictions will be very slow\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå PyTorch not available\")\n",
        "\n",
        "    # Check kernel test status\n",
        "    if not global_settings.get('kernels_tested', False):\n",
        "        print(\"\\n‚ö†Ô∏è  WARNING: Kernel preflight test not run!\")\n",
        "        print(\"   Running with --no_kernels by default for safety\")\n",
        "        global_settings['use_no_kernels'] = True\n",
        "\n",
        "    use_no_kernels_flag = global_settings.get('use_no_kernels', True)\n",
        "    print(f\"\\nüîß Kernel mode: {'--no_kernels' if use_no_kernels_flag else 'WITH kernels'}\")\n",
        "\n",
        "    if use_no_kernels_flag:\n",
        "        print(\"   (Using CPU fallback - slower but more compatible)\")\n",
        "    else:\n",
        "        print(\"   (Using CUDA kernels - faster performance)\")\n",
        "\n",
        "    # Helper functions (same as before)\n",
        "    def find_or_create_folder(drive, folder_name, parent_id='root'):\n",
        "        if not drive:\n",
        "            return None\n",
        "        try:\n",
        "            file_list = drive.ListFile({\n",
        "                'q': f\"title='{folder_name}' and '{parent_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
        "            }).GetList()\n",
        "            if file_list:\n",
        "                print(f\"‚úÖ Found existing folder: {folder_name}\")\n",
        "                return file_list[0]['id']\n",
        "            else:\n",
        "                folder = drive.CreateFile({\n",
        "                    'title': folder_name,\n",
        "                    'mimeType': 'application/vnd.google-apps.folder',\n",
        "                    'parents': [{'id': parent_id}]\n",
        "                })\n",
        "                folder.Upload()\n",
        "                print(f\"‚úÖ Created new folder: {folder_name}\")\n",
        "                return folder['id']\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error with folder '{folder_name}': {e}\")\n",
        "            return None\n",
        "\n",
        "    def upload_to_gdrive(drive, file_path, folder_id, job_name):\n",
        "        if not drive or not os.path.exists(file_path):\n",
        "            return None\n",
        "        try:\n",
        "            uploaded_file = drive.CreateFile({\n",
        "                'title': os.path.basename(file_path),\n",
        "                'parents': [{'id': folder_id}]\n",
        "            })\n",
        "            uploaded_file.SetContentFile(file_path)\n",
        "            uploaded_file.Upload()\n",
        "            file_url = f\"https://drive.google.com/file/d/{uploaded_file['id']}/view\"\n",
        "            print(f\"  ‚òÅÔ∏è  Uploaded to Google Drive: {file_url}\")\n",
        "            return file_url\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è  Upload failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_results_zip(job_dir, output_filename):\n",
        "        with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            results_dirs = [d for d in os.listdir(job_dir) if d.startswith('boltz_results_')]\n",
        "            if results_dirs:\n",
        "                predictions_dir = os.path.join(job_dir, results_dirs[0])\n",
        "                if os.path.exists(predictions_dir):\n",
        "                    for root, dirs, files in os.walk(predictions_dir):\n",
        "                        for file in files:\n",
        "                            file_path = os.path.join(root, file)\n",
        "                            arc_path = os.path.relpath(file_path, predictions_dir)\n",
        "                            zipf.write(file_path, arc_path)\n",
        "\n",
        "    def run_single_prediction(job, settings, job_num, total_jobs):\n",
        "        \"\"\"Run a single Boltz-2 prediction with improved diagnostics\"\"\"\n",
        "        job_start_time = time.time()\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üöÄ Job {job_num}/{total_jobs}: {job['name']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        job_name = job['name']\n",
        "        job_dir = job_name\n",
        "        os.makedirs(job_dir, exist_ok=True)\n",
        "\n",
        "        # Generate YAML file\n",
        "        yaml_content = settings['processor']._generate_yaml(job)\n",
        "        yaml_file = os.path.join(job_dir, f\"{job_name}.yaml\")\n",
        "\n",
        "        with open(yaml_file, 'w') as f:\n",
        "            f.write(yaml_content)\n",
        "\n",
        "        print(f\"üìù Generated YAML configuration\")\n",
        "\n",
        "        # Build Boltz command - USE PREFLIGHT RESULT\n",
        "        cmd_parts = [\n",
        "            \"boltz\", \"predict\", yaml_file,\n",
        "            \"--out_dir\", job_dir,\n",
        "            \"--recycling_steps\", str(settings.get('recycling_steps', 6)),\n",
        "            \"--sampling_steps\", str(settings.get('sampling_steps', 200)),\n",
        "            \"--diffusion_samples\", str(settings.get('diffusion_samples', 5)),\n",
        "            \"--max_parallel_samples\", str(settings.get('max_parallel_samples', 5)),\n",
        "            \"--step_scale\", str(settings.get('step_scale', 1.638)),\n",
        "            \"--output_format\", settings.get('output_format', 'mmcif'),\n",
        "            \"--max_msa_seqs\", str(settings.get('max_msa_seqs', 8192)),\n",
        "            \"--override\"\n",
        "        ]\n",
        "\n",
        "        # Conditionally add --no_kernels based on preflight test\n",
        "        if settings.get('use_no_kernels', True):\n",
        "            cmd_parts.append(\"--no_kernels\")\n",
        "\n",
        "        # Add MSA server if configured\n",
        "        if settings.get('use_msa_server', True):\n",
        "            cmd_parts.extend([\n",
        "                \"--use_msa_server\",\n",
        "                \"--msa_server_url\", settings.get('msa_server_url', 'https://api.colabfold.com'),\n",
        "                \"--msa_pairing_strategy\", settings.get('msa_pairing_strategy', 'greedy')\n",
        "            ])\n",
        "\n",
        "        # Add optional flags\n",
        "        if settings.get('write_full_pae', False):\n",
        "            cmd_parts.append(\"--write_full_pae\")\n",
        "        if settings.get('write_full_pde', False):\n",
        "            cmd_parts.append(\"--write_full_pde\")\n",
        "        if settings.get('predict_affinity', False):\n",
        "            cmd_parts.extend([\n",
        "                \"--predict_affinity\",\n",
        "                \"--sampling_steps_affinity\", str(settings.get('sampling_steps_affinity', 200)),\n",
        "                \"--diffusion_samples_affinity\", str(settings.get('diffusion_samples_affinity', 5))\n",
        "            ])\n",
        "            if settings.get('affinity_mw_correction', False):\n",
        "                cmd_parts.append(\"--affinity_mw_correction\")\n",
        "\n",
        "        cmd = \" \".join(cmd_parts)\n",
        "        print(f\"üîß Command: {cmd}\")\n",
        "\n",
        "        # Run prediction with proper stderr/stdout capture\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                cmd,\n",
        "                shell=True,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=7200  # 2 hour timeout\n",
        "            )\n",
        "\n",
        "            # CRITICAL: Always show stderr if present, even with returncode 0\n",
        "            if result.stderr and result.stderr.strip():\n",
        "                print(f\"\\nüìã Boltz output/warnings:\")\n",
        "                # Show last 50 lines of stderr\n",
        "                stderr_lines = result.stderr.strip().split('\\n')\n",
        "                for line in stderr_lines[-50:]:\n",
        "                    if line.strip():\n",
        "                        print(f\"   {line}\")\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                # Check for output files\n",
        "                results_dirs = [d for d in os.listdir(job_dir) if d.startswith('boltz_results_')]\n",
        "\n",
        "                if not results_dirs:\n",
        "                    print(f\"\\n‚ùå No results directory found\")\n",
        "                    print(f\"   Expected directory starting with 'boltz_results_' in {job_dir}\")\n",
        "                    print(f\"   Actual contents: {os.listdir(job_dir)}\")\n",
        "                    return False\n",
        "\n",
        "                predictions_dir = os.path.join(job_dir, results_dirs[0])\n",
        "\n",
        "                # Count structure files\n",
        "                structure_count = 0\n",
        "                structure_files = []\n",
        "                for root, dirs, files in os.walk(predictions_dir):\n",
        "                    for f in files:\n",
        "                        if f.endswith(('.cif', '.pdb', '.mmcif')):\n",
        "                            structure_count += 1\n",
        "                            structure_files.append(os.path.join(root, f))\n",
        "\n",
        "                if structure_count == 0:\n",
        "                    print(f\"\\n‚ùå No structure files generated\")\n",
        "                    print(f\"   Checked directory: {predictions_dir}\")\n",
        "                    print(f\"   Directory contents:\")\n",
        "                    for root, dirs, files in os.walk(predictions_dir):\n",
        "                        level = root.replace(predictions_dir, '').count(os.sep)\n",
        "                        indent = ' ' * 2 * level\n",
        "                        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "                        sub_indent = ' ' * 2 * (level + 1)\n",
        "                        for f in files:\n",
        "                            print(f\"{sub_indent}{f}\")\n",
        "\n",
        "                    # Show full stderr for debugging\n",
        "                    if result.stderr:\n",
        "                        print(f\"\\nüîç Full Boltz stderr for debugging:\")\n",
        "                        print(result.stderr)\n",
        "\n",
        "                    return False\n",
        "\n",
        "                print(f\"‚úÖ Generated {structure_count} structure files\")\n",
        "                for sf in structure_files[:5]:  # Show first 5\n",
        "                    print(f\"   üìÑ {os.path.basename(sf)}\")\n",
        "                if structure_count > 5:\n",
        "                    print(f\"   ... and {structure_count - 5} more files\")\n",
        "\n",
        "                # Create results zip\n",
        "                zip_filename = f\"{job_name}_results.zip\"\n",
        "                create_results_zip(job_dir, zip_filename)\n",
        "                print(f\"üì¶ Created: {zip_filename}\")\n",
        "\n",
        "                # Upload to Google Drive\n",
        "                if global_settings.get('drive') and gdrive_folder_id:\n",
        "                    upload_url = upload_to_gdrive(\n",
        "                        global_settings['drive'],\n",
        "                        zip_filename,\n",
        "                        gdrive_folder_id,\n",
        "                        job_name\n",
        "                    )\n",
        "                    if upload_url:\n",
        "                        uploaded_files.append({'job': job_name, 'url': upload_url})\n",
        "\n",
        "                # Cleanup\n",
        "                try:\n",
        "                    shutil.rmtree(job_dir)\n",
        "                    if os.path.exists(zip_filename):\n",
        "                        os.remove(zip_filename)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è  Cleanup warning: {e}\")\n",
        "\n",
        "                job_duration = time.time() - job_start_time\n",
        "                print(f\"‚è±Ô∏è  Completed in {job_duration:.1f}s\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"\\n‚ùå Prediction failed (return code: {result.returncode})\")\n",
        "                if result.stderr:\n",
        "                    print(f\"\\nüîç Error details:\")\n",
        "                    print(result.stderr[-1000:])  # Last 1000 chars\n",
        "                return False\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"‚è∞ Prediction timed out (2 hour limit)\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"üí• Error: {e}\")\n",
        "            return False\n",
        "\n",
        "    # Setup Google Drive folder\n",
        "    gdrive_folder_id = None\n",
        "    uploaded_files = []\n",
        "\n",
        "    if global_settings.get('drive'):\n",
        "        gdrive_folder_id = find_or_create_folder(\n",
        "            global_settings['drive'],\n",
        "            global_settings['gdrive_folder_name']\n",
        "        )\n",
        "\n",
        "    # Run batch predictions\n",
        "    batch_jobs = global_settings['batch_jobs']\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üöÄ STARTING BATCH PROCESSING\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total jobs: {len(batch_jobs)}\")\n",
        "    print(f\"Started: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    successful_jobs = 0\n",
        "    failed_jobs = []\n",
        "\n",
        "    for i, job in enumerate(batch_jobs, 1):\n",
        "        success = run_single_prediction(\n",
        "            job,\n",
        "            global_settings,\n",
        "            job_num=i,\n",
        "            total_jobs=len(batch_jobs)\n",
        "        )\n",
        "\n",
        "        if success:\n",
        "            successful_jobs += 1\n",
        "        else:\n",
        "            failed_jobs.append(job['name'])\n",
        "\n",
        "        # Clear GPU cache between jobs\n",
        "        try:\n",
        "            import torch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Final summary\n",
        "    end_time = datetime.now()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä BATCH PROCESSING COMPLETE\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"‚è±Ô∏è  Total duration: {duration}\")\n",
        "    print(f\"‚úÖ Successful: {successful_jobs}/{len(batch_jobs)} jobs\")\n",
        "\n",
        "    if failed_jobs:\n",
        "        print(f\"‚ùå Failed jobs: {len(failed_jobs)}\")\n",
        "        for job_name in failed_jobs:\n",
        "            print(f\"  ‚Ä¢ {job_name}\")\n",
        "\n",
        "    if uploaded_files:\n",
        "        print(f\"\\n‚òÅÔ∏è  Files uploaded to Google Drive: {len(uploaded_files)}\")\n",
        "        print(f\"üìÅ Folder: {global_settings['gdrive_folder_name']}\")\n",
        "        for file_info in uploaded_files[:5]:\n",
        "            print(f\"  ‚Ä¢ {file_info['job']}\")\n",
        "        if len(uploaded_files) > 5:\n",
        "            print(f\"  ... and {len(uploaded_files) - 5} more files\")\n",
        "\n",
        "    print(f\"{'='*60}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOWcdBeOT/Zd3/GC7+t8fVq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}