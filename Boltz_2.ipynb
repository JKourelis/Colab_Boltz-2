{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JKourelis/Colab_Boltz-2/blob/main/Boltz_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqNojEcK_HM1"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/jwohlwend/boltz/main/docs/boltz2_title.png\" height=\"200\" align=\"right\" style=\"height:240px\">\n",
        "\n",
        "## Boltz-2: Democratizing Biomolecular Interaction Modeling\n",
        "\n",
        "Easy to use protein structure and binding affinity prediction using [Boltz-2](https://doi.org/10.1101/2025.06.14.659707). Boltz-2 is a biomolecular foundation model that jointly models complex structures and binding affinities, approaching [AlphaFold3](https://www.nature.com/articles/s41586-024-07487-w) accuracy while running 1000x faster than physics-based methods.\n",
        "\n",
        "**Key Features:**\n",
        "- **Structure Prediction**: Protein, DNA, RNA, and ligand complexes with AlphaFold3-level accuracy\n",
        "- **Binding Affinity**: First deep learning model to approach FEP accuracy for drug discovery\n",
        "- **Open Source**: MIT license for academic and commercial use\n",
        "- **Fast**: 1000x faster than traditional physics-based methods\n",
        "\n",
        "**Usage Options:**\n",
        "1. **Manual Input**: Enter sequences directly in the configuration boxes below\n",
        "2. **FASTA Upload**: Upload FASTA files for batch processing\n",
        "\n",
        "**Citations:**\n",
        "\n",
        "[Wohlwend J, Corso G, Passaro S, et al. Boltz-1: Democratizing Biomolecular Interaction Modeling. *bioRxiv*, 2024](https://doi.org/10.1101/2024.11.19.624167)\n",
        "\n",
        "[Passaro S, Corso G, Wohlwend J, et al. Boltz-2: Towards Accurate and Efficient Binding Affinity Prediction. *bioRxiv*, 2025](https://doi.org/10.1101/2025.06.14.659707)\n",
        "\n",
        "If using automatic MSA generation: [Mirdita M, Sch√ºtze K, Moriwaki Y, et al. ColabFold: making protein folding accessible to all. *Nature Methods*, 2022](https://doi.org/10.1038/s41592-022-01488-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Lo35v692ER",
        "outputId": "b9920012-c2a1-41f2-e0ef-528941fd600a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Removing existing installations]\n",
            "OK\n",
            "[Installing compatible PyTorch+torchvision]\n",
            "OK\n",
            "[Installing lightning stack]\n",
            "OK\n",
            "[Installing boltz]\n",
            "OK\n",
            "[Testing boltz]\n",
            "SUCCESS\n",
            "\n",
            "Boltz-2 installation complete\n",
            "CPU times: user 778 ms, sys: 139 ms, total: 917 ms\n",
            "Wall time: 5min 12s\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 1: Install Boltz-2 and Dependencies\n",
        "%%time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def run_cmd(cmd, desc):\n",
        "    print(f\"[{desc}]\")\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"FAILED: {result.stderr[:200]}\")\n",
        "        return False\n",
        "    print(\"OK\")\n",
        "    return True\n",
        "\n",
        "def install_boltz():\n",
        "    # Complete cleanup\n",
        "    if not run_cmd(\n",
        "        f\"{sys.executable} -m pip uninstall torch torchvision torchaudio pytorch-lightning torchmetrics boltz -y\",\n",
        "        \"Removing existing installations\"\n",
        "    ):\n",
        "        pass  # Continue even if uninstall fails\n",
        "\n",
        "    # Install PyTorch\n",
        "    if not run_cmd(\n",
        "        f\"{sys.executable} -m pip install torch==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu121\",\n",
        "        \"Installing compatible PyTorch+torchvision\"\n",
        "    ):\n",
        "        return False\n",
        "\n",
        "    # Install lightning stack\n",
        "    if not run_cmd(\n",
        "        f\"{sys.executable} -m pip install pytorch-lightning==2.4.0 torchmetrics==1.4.0\",\n",
        "        \"Installing lightning stack\"\n",
        "    ):\n",
        "        return False\n",
        "\n",
        "    # Install boltz\n",
        "    if not run_cmd(\n",
        "        f\"{sys.executable} -m pip install boltz\",\n",
        "        \"Installing boltz\"\n",
        "    ):\n",
        "        return False\n",
        "\n",
        "    # Test installation\n",
        "    print(\"[Testing boltz]\")\n",
        "    test_result = subprocess.run([\"boltz\", \"--help\"], capture_output=True, text=True, timeout=30)\n",
        "\n",
        "    if test_result.returncode == 0:\n",
        "        print(\"SUCCESS\")\n",
        "        with open(\"/content/BOLTZ_READY\", \"w\") as f:\n",
        "            f.write(\"Ready\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"FAILED:\")\n",
        "        print(test_result.stderr)\n",
        "        return False\n",
        "\n",
        "# Execute installation\n",
        "if install_boltz():\n",
        "    print(\"\\nBoltz-2 installation complete\")\n",
        "else:\n",
        "    print(\"\\nInstallation failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwUc04CG-NzL",
        "outputId": "4de30e0b-2791-4487-ea5e-b6957cd335d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Selected input method: FASTA Upload\n",
            "üìÅ Next: Upload your FASTA files in the FASTA Upload cell\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 2: Choose Input Method\n",
        "input_method = \"FASTA Upload\" #@param [\"Manual Input\", \"FASTA Upload\"]\n",
        "#@markdown **Manual Input**: Enter sequences directly in the next cell\n",
        "#@markdown **FASTA Upload**: Upload FASTA files for batch processing\n",
        "\n",
        "# Initialize global variables\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "# Global settings that will be used throughout\n",
        "global_settings = {\n",
        "    'input_method': input_method,\n",
        "    'sequences': [],\n",
        "    'batch_jobs': [],\n",
        "    'drive': None,\n",
        "    'gdrive_folder_name': \"Boltz2_Predictions\",\n",
        "    'final_jobname': None\n",
        "}\n",
        "\n",
        "def add_hash(x, y):\n",
        "    return x + \"_\" + hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "print(f\"‚úÖ Selected input method: {input_method}\")\n",
        "if input_method == \"Manual Input\":\n",
        "    print(\"üìù Next: Configure your sequences in the Manual Input cell\")\n",
        "else:\n",
        "    print(\"üìÅ Next: Upload your FASTA files in the FASTA Upload cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcbddZPH52WA",
        "outputId": "57f6af86-c66e-49d0-8f41-a9e76448b7bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è  Please run the 'Choose Input Method' cell first\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 3: Manual Input Configuration (Skip if using FASTA Upload)\n",
        "#@markdown Only run this cell if you selected \"Manual Input\" above\n",
        "\n",
        "# Job configuration\n",
        "jobname = '' #@param {type:\"string\"}\n",
        "#@markdown - Job name for output files\n",
        "\n",
        "# Google Drive setup\n",
        "setup_google_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown - Setup Google Drive for automatic result upload\n",
        "gdrive_folder_name = \"Boltz2_Predictions\" #@param {type:\"string\"}\n",
        "#@markdown - Google Drive folder name\n",
        "\n",
        "# Sequence inputs\n",
        "seq1_name = 'A' #@param {type:\"string\"}\n",
        "seq1_type = \"protein\" #@param [\"protein\", \"dna\", \"rna\", \"smiles\", \"ccd\"]\n",
        "seq1_content = '' #@param {type:\"string\"}\n",
        "seq1_copies = 1 #@param {type:\"integer\"}\n",
        "\n",
        "seq2_name = 'B' #@param {type:\"string\"}\n",
        "seq2_type = \"protein\" #@param [\"protein\", \"dna\", \"rna\", \"smiles\", \"ccd\"]\n",
        "seq2_content = '' #@param {type:\"string\"}\n",
        "seq2_copies = 1 #@param {type:\"integer\"}\n",
        "\n",
        "seq3_name = 'C' #@param {type:\"string\"}\n",
        "seq3_type = \"protein\" #@param [\"protein\", \"dna\", \"rna\", \"smiles\", \"ccd\"]\n",
        "seq3_content = '' #@param {type:\"string\"}\n",
        "seq3_copies = 1 #@param {type:\"integer\"}\n",
        "\n",
        "# Check if this cell should run\n",
        "if 'global_settings' not in globals():\n",
        "    print(\"‚ö†Ô∏è  Please run the 'Choose Input Method' cell first\")\n",
        "elif global_settings['input_method'] != \"Manual Input\":\n",
        "    print(\"‚è≠Ô∏è  Skipping manual input (FASTA Upload selected)\")\n",
        "else:\n",
        "    # Setup Google Drive if requested\n",
        "    drive = None\n",
        "    if setup_google_drive:\n",
        "        try:\n",
        "            from pydrive2.drive import GoogleDrive\n",
        "            from pydrive2.auth import GoogleAuth\n",
        "            from google.colab import auth\n",
        "            from oauth2client.client import GoogleCredentials\n",
        "            from google.colab import files\n",
        "\n",
        "            print(\"Setting up Google Drive...\")\n",
        "            auth.authenticate_user()\n",
        "            gauth = GoogleAuth()\n",
        "            gauth.credentials = GoogleCredentials.get_application_default()\n",
        "            drive = GoogleDrive(gauth)\n",
        "            print(\"‚úÖ Google Drive connected successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Google Drive setup failed: {e}\")\n",
        "            drive = None\n",
        "\n",
        "    # Process sequences\n",
        "    sequences = []\n",
        "    all_sequences = [\n",
        "        (seq1_name, seq1_type, seq1_content, seq1_copies),\n",
        "        (seq2_name, seq2_type, seq2_content, seq2_copies),\n",
        "        (seq3_name, seq3_type, seq3_content, seq3_copies)\n",
        "    ]\n",
        "\n",
        "    for name, seq_type, content, copies in all_sequences:\n",
        "        if content.strip():  # Only process non-empty sequences\n",
        "            chain_ids = []\n",
        "            for i in range(copies):\n",
        "                if copies == 1:\n",
        "                    chain_ids.append(name)\n",
        "                else:\n",
        "                    chain_ids.append(f\"{name}{i+1}\")\n",
        "\n",
        "            sequences.append({\n",
        "                'name': name,\n",
        "                'type': seq_type,\n",
        "                'content': content.strip(),\n",
        "                'copies': copies,\n",
        "                'chain_ids': chain_ids\n",
        "            })\n",
        "\n",
        "    # Generate jobname hash\n",
        "    if sequences:\n",
        "        sequence_string = \"\".join([seq['content'] for seq in sequences])\n",
        "        final_jobname = add_hash(jobname.replace(' ', '_'), sequence_string)\n",
        "\n",
        "        # Update global settings\n",
        "        global_settings.update({\n",
        "            'sequences': sequences,\n",
        "            'drive': drive,\n",
        "            'gdrive_folder_name': gdrive_folder_name,\n",
        "            'final_jobname': final_jobname\n",
        "        })\n",
        "\n",
        "        print(\"‚úÖ Manual sequences configured:\")\n",
        "        print(f\"üìÅ Job name: {final_jobname}\")\n",
        "        for seq in sequences:\n",
        "            print(f\"  {seq['name']}: {seq['type']}, {seq['copies']} copies, chains: {seq['chain_ids']}\")\n",
        "            print(f\"    Content: {seq['content'][:50]}{'...' if len(seq['content']) > 50 else ''}\")\n",
        "    else:\n",
        "        print(\"‚ùå No sequences provided\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leGpFbW1Qz3L",
        "outputId": "921683df-2175-401a-ed61-4dd353e7d962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è  Please run the 'Choose Input Method' cell first\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 4: FASTA Upload Configuration (Skip if using Manual Input)\n",
        "#@markdown Only run this cell if you selected \"FASTA Upload\" above\n",
        "\n",
        "# Google Drive setup for FASTA upload\n",
        "setup_google_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown - Setup Google Drive for automatic result upload\n",
        "gdrive_folder_name = \"Boltz2_Predictions\" #@param {type:\"string\"}\n",
        "#@markdown - Google Drive folder name\n",
        "\n",
        "upload_fasta1 = True #@param {type:\"boolean\"}\n",
        "#@markdown - Upload FASTA file 1\n",
        "upload_fasta2 = True #@param {type:\"boolean\"}\n",
        "#@markdown - Upload FASTA file 2 (for combinations)\n",
        "predict_combinations = True #@param {type:\"boolean\"}\n",
        "#@markdown - Predict all combinations between FASTA1 and FASTA2 sequences\n",
        "\n",
        "# Check if this cell should run\n",
        "if 'global_settings' not in globals():\n",
        "    print(\"‚ö†Ô∏è  Please run the 'Choose Input Method' cell first\")\n",
        "elif global_settings['input_method'] != \"FASTA Upload\":\n",
        "    print(\"‚è≠Ô∏è  Skipping FASTA upload (Manual Input selected)\")\n",
        "else:\n",
        "    import re\n",
        "    from google.colab import files\n",
        "\n",
        "    # Setup Google Drive if requested\n",
        "    drive = None\n",
        "    if setup_google_drive:\n",
        "        try:\n",
        "            from pydrive2.drive import GoogleDrive\n",
        "            from pydrive2.auth import GoogleAuth\n",
        "            from google.colab import auth\n",
        "            from oauth2client.client import GoogleCredentials\n",
        "\n",
        "            print(\"Setting up Google Drive...\")\n",
        "            auth.authenticate_user()\n",
        "            gauth = GoogleAuth()\n",
        "            gauth.credentials = GoogleCredentials.get_application_default()\n",
        "            drive = GoogleDrive(gauth)\n",
        "            print(\"‚úÖ Google Drive connected successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Google Drive setup failed: {e}\")\n",
        "            drive = None\n",
        "\n",
        "    # FASTA processing functions\n",
        "    def parse_fasta(file_content, file_name):\n",
        "        \"\"\"Parse FASTA file content and return list of sequences.\"\"\"\n",
        "        sequences = []\n",
        "        current_seq = \"\"\n",
        "        current_id = \"\"\n",
        "\n",
        "        for line in file_content.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if line.startswith('>'):\n",
        "                if current_seq and current_id:\n",
        "                    clean_id = re.sub(r'[^\\w\\-_]', '_', current_id)\n",
        "                    sequences.append({\n",
        "                        'id': clean_id,\n",
        "                        'original_id': current_id,\n",
        "                        'sequence': current_seq,\n",
        "                        'source_file': file_name\n",
        "                    })\n",
        "                current_id = line[1:]  # Remove '>'\n",
        "                current_seq = \"\"\n",
        "            else:\n",
        "                current_seq += line\n",
        "\n",
        "        # Add last sequence\n",
        "        if current_seq and current_id:\n",
        "            clean_id = re.sub(r'[^\\w\\-_]', '_', current_id)\n",
        "            sequences.append({\n",
        "                'id': clean_id,\n",
        "                'original_id': current_id,\n",
        "                'sequence': current_seq,\n",
        "                'source_file': file_name\n",
        "            })\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    # Process FASTA uploads\n",
        "    fasta1_sequences = []\n",
        "    fasta2_sequences = []\n",
        "\n",
        "    if upload_fasta1:\n",
        "        print(\"Upload FASTA file 1:\")\n",
        "        uploaded_fasta1 = files.upload()\n",
        "        for filename, content in uploaded_fasta1.items():\n",
        "            file_content = content.decode('utf-8')\n",
        "            fasta1_sequences = parse_fasta(file_content, filename)\n",
        "            print(f\"FASTA1 loaded: {len(fasta1_sequences)} sequences from {filename}\")\n",
        "\n",
        "    if upload_fasta2:\n",
        "        print(\"Upload FASTA file 2:\")\n",
        "        uploaded_fasta2 = files.upload()\n",
        "        for filename, content in uploaded_fasta2.items():\n",
        "            file_content = content.decode('utf-8')\n",
        "            fasta2_sequences = parse_fasta(file_content, filename)\n",
        "            print(f\"FASTA2 loaded: {len(fasta2_sequences)} sequences from {filename}\")\n",
        "\n",
        "    # Generate batch jobs\n",
        "    batch_jobs = []\n",
        "    if predict_combinations and fasta1_sequences and fasta2_sequences:\n",
        "        print(f\"Generating {len(fasta1_sequences)} x {len(fasta2_sequences)} = {len(fasta1_sequences) * len(fasta2_sequences)} combinations\")\n",
        "\n",
        "        for seq1 in fasta1_sequences:\n",
        "            for seq2 in fasta2_sequences:\n",
        "                job_name = f\"{seq1['id']}_{seq2['id']}\"\n",
        "                batch_jobs.append({\n",
        "                    'name': job_name,\n",
        "                    'sequences': [\n",
        "                        {'id': 'A', 'type': 'protein', 'content': seq1['sequence']},\n",
        "                        {'id': 'B', 'type': 'protein', 'content': seq2['sequence']}\n",
        "                    ]\n",
        "                })\n",
        "\n",
        "    elif fasta1_sequences and not predict_combinations:\n",
        "        print(f\"Processing {len(fasta1_sequences)} individual sequences from FASTA1\")\n",
        "        for seq in fasta1_sequences:\n",
        "            job_name = seq['id']\n",
        "            batch_jobs.append({\n",
        "                'name': job_name,\n",
        "                'sequences': [\n",
        "                    {'id': 'A', 'type': 'protein', 'content': seq['sequence']}\n",
        "                ]\n",
        "            })\n",
        "\n",
        "    # Update global settings\n",
        "    global_settings.update({\n",
        "        'batch_jobs': batch_jobs,\n",
        "        'drive': drive,\n",
        "        'gdrive_folder_name': gdrive_folder_name\n",
        "    })\n",
        "\n",
        "    print(f\"‚úÖ FASTA upload configured: {len(batch_jobs)} jobs to process\")\n",
        "    for i, job in enumerate(batch_jobs[:5]):  # Show first 5\n",
        "        print(f\"  Job {i+1}: {job['name']}\")\n",
        "    if len(batch_jobs) > 5:\n",
        "        print(f\"  ... and {len(batch_jobs) - 5} more jobs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnrh_B6Y92ph",
        "outputId": "eecf6388-f26a-4936-ef15-9ceaba72fe4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ MSA configuration set:\n",
            "  Mode: mmseqs2_uniref_env\n",
            "  Pairing strategy: greedy\n",
            "  Use MSA server: True\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 5: MSA Configuration\n",
        "msa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\n",
        "#@markdown - MSA generation method. mmseqs2 modes use the ColabFold server\n",
        "\n",
        "msa_pairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"]\n",
        "#@markdown - `greedy` = pair any taxonomically matching subsets, `complete` = all sequences must match\n",
        "\n",
        "# Check if global_settings exists\n",
        "if 'global_settings' not in globals():\n",
        "    print(\"‚ö†Ô∏è  Please run the 'Choose Input Method' cell first\")\n",
        "else:\n",
        "    # Configure MSA settings based on mode\n",
        "    if \"mmseqs2\" in msa_mode:\n",
        "        use_msa_server = True\n",
        "        msa_server_url = \"https://api.colabfold.com\"\n",
        "    else:\n",
        "        use_msa_server = False\n",
        "        msa_server_url = None\n",
        "\n",
        "    # Handle custom MSA upload if selected\n",
        "    if msa_mode == \"custom\":\n",
        "        print(\"Upload your custom MSA file (A3M format):\")\n",
        "        from google.colab import files\n",
        "        custom_msa_dict = files.upload()\n",
        "        if custom_msa_dict:\n",
        "            custom_msa_file = list(custom_msa_dict.keys())[0]\n",
        "            print(f\"Custom MSA uploaded: {custom_msa_file}\")\n",
        "        else:\n",
        "            print(\"No custom MSA uploaded, switching to single_sequence mode\")\n",
        "            msa_mode = \"single_sequence\"\n",
        "            use_msa_server = False\n",
        "\n",
        "    # Store MSA settings in global_settings\n",
        "    global_settings.update({\n",
        "        'msa_mode': msa_mode,\n",
        "        'msa_pairing_strategy': msa_pairing_strategy,\n",
        "        'use_msa_server': use_msa_server,\n",
        "        'msa_server_url': msa_server_url\n",
        "    })\n",
        "\n",
        "    print(f\"‚úÖ MSA configuration set:\")\n",
        "    print(f\"  Mode: {msa_mode}\")\n",
        "    print(f\"  Pairing strategy: {msa_pairing_strategy}\")\n",
        "    print(f\"  Use MSA server: {use_msa_server}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3am6YdJ92zA",
        "outputId": "818237bc-091f-4249-9e39-a777f7cd06ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Advanced settings configured:\n",
            "  Recycling steps: 6\n",
            "  Sampling steps: 200\n",
            "  Diffusion samples: 5\n",
            "  Predict affinity: False\n",
            "  Output format: mmcif\n",
            "  Use potentials: True\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 6: Advanced Prediction Settings\n",
        "# Structure Prediction Settings\n",
        "recycling_steps = 6 #@param {type:\"integer\"}\n",
        "#@markdown - **Iterative refinement passes**: Each cycle refines the structure using updated predictions. Higher values improve local geometry and confidence scores. **Time**: ~linear scaling (3 steps = 3x base time). **VRAM**: +20-30% per additional step for intermediate states.\n",
        "\n",
        "sampling_steps = 200 #@param {type:\"integer\"}\n",
        "#@markdown - **Diffusion denoising iterations**: Controls how many steps the diffusion model takes to generate structures from noise. More steps = smoother, higher quality structures. **Time**: Linear scaling (50 steps = 4x faster than 200). **VRAM**: +10-15% for intermediate diffusion states.\n",
        "\n",
        "diffusion_samples = 5 #@param {type:\"integer\"}\n",
        "#@markdown - **Independent structure predictions**: Number of different structures generated per input. More samples increase diversity and reliability of results. **Time**: Linear scaling (5 samples = 5x base time). **VRAM**: Depends on max_parallel_samples setting.\n",
        "\n",
        "max_parallel_samples = 5 #@param {type:\"integer\"}\n",
        "#@markdown - **GPU memory management**: How many diffusion samples are processed simultaneously. Critical for large complexes - each parallel sample requires full model memory allocation. **Time**: Minimal impact on total time. **VRAM**: ~Linear scaling (2 parallel = ~2x memory, 5 parallel = ~5x memory).\n",
        "\n",
        "step_scale = 1.638 #@param {type:\"number\"}\n",
        "#@markdown - **Sampling temperature**: Controls randomness in structure generation. Higher values increase diversity but may reduce quality. 1.638 is optimized default. **Time**: No impact. **VRAM**: No impact.\n",
        "\n",
        "# Affinity Prediction Settings\n",
        "predict_affinity = False #@param {type:\"boolean\"}\n",
        "#@markdown - **Binding strength prediction**: Runs additional affinity model to predict binding strength (Kd/Ki values). Most reliable for protein-small molecule complexes. **Time**: +50-100% total time. **VRAM**: +40-60% for affinity model loading.\n",
        "\n",
        "affinity_mw_correction = False #@param {type:\"boolean\"}\n",
        "#@markdown - **Molecular weight adjustment**: Applies size-based corrections to affinity predictions. Only affects affinity calculation, not structure. **Time**: Minimal impact. **VRAM**: No impact.\n",
        "\n",
        "sampling_steps_affinity = 200 #@param {type:\"integer\"}\n",
        "#@markdown - **Affinity model diffusion steps**: Controls quality of affinity predictions. Similar to sampling_steps but for the affinity model. **Time**: Linear scaling within affinity prediction. **VRAM**: +5-10% for affinity diffusion states.\n",
        "\n",
        "diffusion_samples_affinity = 5 #@param {type:\"integer\"}\n",
        "#@markdown - **Affinity prediction ensemble size**: Number of independent affinity predictions to average for final binding strength. More samples = more reliable Kd estimates. **Time**: Linear scaling for affinity portion. **VRAM**: Minimal additional impact.\n",
        "\n",
        "# Output and Optimization Settings\n",
        "output_format = \"mmcif\" #@param [\"mmcif\", \"pdb\"]\n",
        "#@markdown - **Structure file format**: mmCIF supports more metadata and modern features, PDB is more widely compatible. Both contain same structural information. **Time**: No impact. **VRAM**: No impact.\n",
        "\n",
        "write_full_pae = False #@param {type:\"boolean\"}\n",
        "#@markdown - **Save Predicted Aligned Error matrix**: Confidence scores between all residue pairs. Essential for assessing interface quality and domain reliability. **Time**: +5-10% for matrix computation and I/O. **VRAM**: +10-20% for large complexes during matrix storage.\n",
        "\n",
        "write_full_pde = False #@param {type:\"boolean\"}\n",
        "#@markdown - **Save Predicted Distance Error matrix**: Distance confidence predictions between residue pairs. Useful for validation and uncertainty quantification. **Time**: +5-10% for matrix computation and I/O. **VRAM**: +10-20% for large complexes during matrix storage.\n",
        "\n",
        "use_potentials = True #@param {type:\"boolean\"}\n",
        "#@markdown - **Inference-time physics optimization**: Applies physics-based energy minimization to improve local geometry and remove clashes. Significantly improves structure quality, especially for interfaces. **Time**: +30-50% total time. **VRAM**: +15-25% for physics calculation buffers.\n",
        "\n",
        "# Check if global_settings exists\n",
        "if 'global_settings' not in globals():\n",
        "    print(\"‚ö†Ô∏è  Please run the 'Choose Input Method' cell first\")\n",
        "else:\n",
        "    # Store advanced settings\n",
        "    advanced_settings = {\n",
        "        'recycling_steps': recycling_steps,\n",
        "        'sampling_steps': sampling_steps,\n",
        "        'diffusion_samples': diffusion_samples,\n",
        "        'max_parallel_samples': max_parallel_samples,\n",
        "        'step_scale': step_scale,\n",
        "        'predict_affinity': predict_affinity,\n",
        "        'affinity_mw_correction': affinity_mw_correction,\n",
        "        'sampling_steps_affinity': sampling_steps_affinity,\n",
        "        'diffusion_samples_affinity': diffusion_samples_affinity,\n",
        "        'output_format': output_format,\n",
        "        'write_full_pae': write_full_pae,\n",
        "        'write_full_pde': write_full_pde,\n",
        "        'use_potentials': use_potentials,\n",
        "        'max_msa_seqs': 8192,\n",
        "        'subsample_msa': False,\n",
        "        'num_subsampled_msa': 1024\n",
        "    }\n",
        "\n",
        "    global_settings.update(advanced_settings)\n",
        "\n",
        "    print(\"‚úÖ Advanced settings configured:\")\n",
        "    print(f\"  Recycling steps: {recycling_steps}\")\n",
        "    print(f\"  Sampling steps: {sampling_steps}\")\n",
        "    print(f\"  Diffusion samples: {diffusion_samples}\")\n",
        "    print(f\"  Predict affinity: {predict_affinity}\")\n",
        "    print(f\"  Output format: {output_format}\")\n",
        "    print(f\"  Use potentials: {use_potentials}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srPcPZupIjUh",
        "outputId": "d75c9961-32b4-47b0-e669-157a763e1290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Residue modifications configured: 0 modifications\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 6.1: Residue Modifications Instructions (Optional)\n",
        "#@markdown Specify residue modifications for amino acid, DNA, or RNA sequences. Each row should define one modification, with values separated by colons (:). The format is:\n",
        "#@markdown\n",
        "#@markdown `SEQ_ID : RESIDUE_INDEX : CCD_CODE`\n",
        "#@markdown\n",
        "#@markdown * **SEQ_ID** ‚Üí The chain ID of the sequence as defined in **Input Sequences**.\n",
        "#@markdown * **RESIDUE_INDEX** ‚Üí The residue position to modify. Use **1** for the first residue.\n",
        "#@markdown * **CCD_CODE** ‚Üí The **Chemical Component Dictionary (CCD) code** of the modification.\n",
        "#@markdown\n",
        "#@markdown **Example Input:**\n",
        "#@markdown ```\n",
        "#@markdown A:102:MLY\n",
        "#@markdown B:1:5MC\n",
        "#@markdown C:26:PSU\n",
        "#@markdown ```\n",
        "#@markdown\n",
        "#@markdown **Notes:**\n",
        "#@markdown * Chain IDs (**SEQ_ID**) must match those in **Input Sequences**.\n",
        "#@markdown * Residue indices start at **1**, not **0**.\n",
        "#@markdown * Use valid **CCD codes** for modifications, use this resource for information on which CCD codes to use for your modification: https://pmc.ncbi.nlm.nih.gov/articles/PMC11394121/\n",
        "\n",
        "residue_modifications = '' #@param {type:\"string\"}\n",
        "#@markdown - Enter residue modifications (one per line, format: CHAIN_ID:RESIDUE_INDEX:CCD_CODE)\n",
        "\n",
        "# Process residue modifications\n",
        "modifications_list = []\n",
        "if residue_modifications.strip():\n",
        "    for line in residue_modifications.strip().split('\\n'):\n",
        "        if line.strip():\n",
        "            parts = line.strip().split(':')\n",
        "            if len(parts) == 3:\n",
        "                chain_id, res_idx, ccd_code = parts\n",
        "                modifications_list.append({\n",
        "                    'chain_id': chain_id.strip(),\n",
        "                    'position': int(res_idx.strip()),\n",
        "                    'ccd': ccd_code.strip()\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Invalid modification format: {line}\")\n",
        "\n",
        "print(f\"Residue modifications configured: {len(modifications_list)} modifications\")\n",
        "for mod in modifications_list:\n",
        "    print(f\"  Chain {mod['chain_id']}, position {mod['position']}: {mod['ccd']}\")\n",
        "\n",
        "if 'global_settings' in globals() and modifications_list:\n",
        "    global_settings['modifications_list'] = modifications_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5cXA7_UImSm",
        "outputId": "f4712f1a-6226-4d50-d9db-e47391f1ab12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No pocket restraints configured\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 6.2: Pocket Restraints Instructions (Optional)\n",
        "#@markdown The **Binder Chain** corresponds to the binder chain, while \"Contact Residues\" specifies residues interacting with it.\n",
        "#@markdown Specify inter-chain pocket restraints to guide Boltz-2 in folding complexes. These restraints define interactions between a binder sequence and residues in other sequences, influencing the folding process.\n",
        "#@markdown Each row should define one pocket restraint, with values separated by colons (:). The format is:\n",
        "#@markdown\n",
        "#@markdown `CONTACT_CHAIN:CONTACT_RES`\n",
        "#@markdown\n",
        "#@markdown * **CONTACT_CHAIN** ‚Üí The chain containing the interacting residue.\n",
        "#@markdown * **CONTACT_RES** ‚Üí The position of the residue on **CONTACT_CHAIN**.\n",
        "#@markdown\n",
        "#@markdown **Example Input:**\n",
        "#@markdown ```\n",
        "#@markdown A:66\n",
        "#@markdown A:78\n",
        "#@markdown B:13\n",
        "#@markdown ```\n",
        "#@markdown\n",
        "#@markdown **Notes:**\n",
        "#@markdown * Chain names match those in **Input Sequences**.\n",
        "#@markdown * Residue numbering starts at 1.\n",
        "#@markdown * The model currently only supports a single binder chain per pocket restraint, but multiple contact residues can be specified across different chains.\n",
        "#@markdown * The chain name of the binder should only be specified if pocket restraints are being used.\n",
        "\n",
        "binder_chain = '' #@param {type:\"string\"}\n",
        "#@markdown - Specify the chain acting as the binder. See above instructions for more details.\n",
        "contact_residues = '' #@param {type:\"string\"}\n",
        "#@markdown - Specify residues interacting with the binder chain. See above instructions for more details.\n",
        "\n",
        "# Process pocket restraints\n",
        "pocket_contacts = []\n",
        "if contact_residues.strip() and binder_chain.strip():\n",
        "    for line in contact_residues.strip().split('\\n'):\n",
        "        if line.strip():\n",
        "            parts = line.strip().split(':')\n",
        "            if len(parts) == 2:\n",
        "                contact_chain, contact_res = parts\n",
        "                pocket_contacts.append({\n",
        "                    'chain_id': contact_chain.strip(),\n",
        "                    'residue': int(contact_res.strip())\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Invalid contact format: {line}\")\n",
        "\n",
        "if binder_chain.strip():\n",
        "    print(f\"Pocket restraints configured:\")\n",
        "    print(f\"  Binder chain: {binder_chain.strip()}\")\n",
        "    print(f\"  Contact residues: {len(pocket_contacts)} contacts\")\n",
        "    for contact in pocket_contacts:\n",
        "        print(f\"    Chain {contact['chain_id']}, residue {contact['residue']}\")\n",
        "else:\n",
        "    print(\"No pocket restraints configured\")\n",
        "\n",
        "if 'global_settings' in globals() and binder_chain.strip():\n",
        "    global_settings['binder_chain'] = binder_chain.strip()\n",
        "    global_settings['pocket_contacts'] = pocket_contacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7rSMomHIq8K",
        "outputId": "2888581e-d3cf-43dd-e739-b555f73023c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Covalent restraints configured: 0 bonds\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 6.3: Covalent Restraints Instructions (Optional)\n",
        "#@markdown Specify covalent bonds between atoms to guide Boltz-2 in complex folding. These restraints define fixed interactions between atoms in different sequences, ensuring structural constraints are maintained.\n",
        "#@markdown Each row should define one covalent restraint, with values separated by colons (:). The format is:\n",
        "#@markdown\n",
        "#@markdown `CHAIN_ID1:RES_ID1:ATOM_NAME1:CHAIN_ID2:RES_ID2:ATOM_NAME2`\n",
        "#@markdown\n",
        "#@markdown * **CHAIN_ID1** ‚Üí The chain containing the first atom.\n",
        "#@markdown * **RES_ID1** ‚Üí Residue index on **CHAIN_ID1**.\n",
        "#@markdown * **ATOM_NAME1** ‚Üí Atom name in **RES_ID1**.\n",
        "#@markdown * **CHAIN_ID2** ‚Üí The chain containing the second atom.\n",
        "#@markdown * **RES_ID2** ‚Üí Residue index on **CHAIN_ID2**.\n",
        "#@markdown * **ATOM_NAME2** ‚Üí Atom name in **RES_ID2**.\n",
        "#@markdown\n",
        "#@markdown **Example Input:**\n",
        "#@markdown ```\n",
        "#@markdown A:6:CA:B:26:CB\n",
        "#@markdown C:1:N1:A:45:OG\n",
        "#@markdown ```\n",
        "#@markdown\n",
        "#@markdown **Notes:**\n",
        "#@markdown * Chain names match those in **Input Sequences**.\n",
        "#@markdown * Residue numbering starts at 1.\n",
        "#@markdown * Atom names must match standardized PDB/CIF naming conventions.\n",
        "#@markdown * Only canonical residues and CCD ligands are supported.\n",
        "#@markdown * Covalent restraints ensure atoms remain bonded during folding but do not enforce bond angles or torsions.\n",
        "\n",
        "covalent_restraints = '' #@param {type:\"string\"}\n",
        "#@markdown - Specify covalent bonds between atoms. See above instructions for more details.\n",
        "\n",
        "# Process covalent restraints\n",
        "covalent_bonds = []\n",
        "if covalent_restraints.strip():\n",
        "    for line in covalent_restraints.strip().split('\\n'):\n",
        "        if line.strip():\n",
        "            parts = line.strip().split(':')\n",
        "            if len(parts) == 6:\n",
        "                chain1, res1, atom1, chain2, res2, atom2 = parts\n",
        "                covalent_bonds.append({\n",
        "                    'atom1': [chain1.strip(), int(res1.strip()), atom1.strip()],\n",
        "                    'atom2': [chain2.strip(), int(res2.strip()), atom2.strip()]\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Invalid covalent restraint format: {line}\")\n",
        "\n",
        "print(f\"Covalent restraints configured: {len(covalent_bonds)} bonds\")\n",
        "for bond in covalent_bonds:\n",
        "    print(f\"  {bond['atom1'][0]}:{bond['atom1'][1]}:{bond['atom1'][2]} - {bond['atom2'][0]}:{bond['atom2'][1]}:{bond['atom2'][2]}\")\n",
        "\n",
        "if 'global_settings' in globals() and covalent_bonds:\n",
        "    global_settings['covalent_bonds'] = covalent_bonds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfaAZLH1-6M6",
        "outputId": "ef40c717-ba73-4acc-ab73-84015dfcdeca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Error: Please run the previous configuration cells first\n",
            "CPU times: user 3.44 ms, sys: 0 ns, total: 3.44 ms\n",
            "Wall time: 3.45 ms\n"
          ]
        }
      ],
      "source": [
        "#@title Run Boltz-2 Prediction (Fixed)\n",
        "%%time\n",
        "import subprocess\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if global_settings exists and is properly configured\n",
        "if 'global_settings' not in globals():\n",
        "    print(\"‚ùå Error: Please run the previous configuration cells first\")\n",
        "elif not global_settings.get('sequences') and not global_settings.get('batch_jobs'):\n",
        "    print(\"‚ùå Error: No sequences or batch jobs configured\")\n",
        "    print(\"Please run either Manual Input or FASTA Upload configuration\")\n",
        "else:\n",
        "    # GPU verification\n",
        "    print(\"üîç Checking GPU availability...\")\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB)\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  WARNING: No GPU detected - predictions will be very slow\")\n",
        "    except ImportError:\n",
        "        print(\"‚ùå PyTorch not available\")\n",
        "\n",
        "    # Helper functions\n",
        "    def find_or_create_folder(drive, folder_name, parent_id='root'):\n",
        "        \"\"\"Find existing folder or create new one in Google Drive.\"\"\"\n",
        "        if not drive:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            file_list = drive.ListFile({'q': f\"title='{folder_name}' and '{parent_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false\"}).GetList()\n",
        "\n",
        "            if file_list:\n",
        "                print(f\"‚úÖ Found existing folder: {folder_name}\")\n",
        "                return file_list[0]['id']\n",
        "            else:\n",
        "                folder = drive.CreateFile({\n",
        "                    'title': folder_name,\n",
        "                    'mimeType': 'application/vnd.google-apps.folder',\n",
        "                    'parents': [{'id': parent_id}]\n",
        "                })\n",
        "                folder.Upload()\n",
        "                print(f\"‚úÖ Created new folder: {folder_name}\")\n",
        "                return folder['id']\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error with folder '{folder_name}': {e}\")\n",
        "            return None\n",
        "\n",
        "    def upload_to_gdrive(drive, file_path, folder_id, job_name):\n",
        "        \"\"\"Upload file to Google Drive folder.\"\"\"\n",
        "        if not drive or not os.path.exists(file_path):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            uploaded_file = drive.CreateFile({\n",
        "                'title': os.path.basename(file_path),\n",
        "                'parents': [{'id': folder_id}]\n",
        "            })\n",
        "            uploaded_file.SetContentFile(file_path)\n",
        "            uploaded_file.Upload()\n",
        "\n",
        "            file_url = f\"https://drive.google.com/file/d/{uploaded_file['id']}/view\"\n",
        "            print(f\"‚úÖ Uploaded {job_name} to Google Drive: {file_url}\")\n",
        "            return file_url\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Upload failed for {job_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_results_zip(job_dir, output_filename):\n",
        "        \"\"\"Create a zip file with only the prediction outputs.\"\"\"\n",
        "        with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            # Find actual results directory\n",
        "            results_dirs = [d for d in os.listdir(job_dir) if d.startswith('boltz_results_')]\n",
        "            if results_dirs:\n",
        "                predictions_dir = os.path.join(job_dir, results_dirs[0])\n",
        "                if os.path.exists(predictions_dir):\n",
        "                    # Add all files from results directory directly to zip root\n",
        "                    for root, dirs, files in os.walk(predictions_dir):\n",
        "                        for file in files:\n",
        "                            file_path = os.path.join(root, file)\n",
        "                            # Use relative path from predictions_dir as the archive path\n",
        "                            arc_path = os.path.relpath(file_path, predictions_dir)\n",
        "                            zipf.write(file_path, arc_path)\n",
        "\n",
        "    def run_single_prediction(job_name, sequences_data, settings, is_batch=False, job_num=0, total_jobs=0):\n",
        "        \"\"\"Run a single Boltz-2 prediction.\"\"\"\n",
        "        job_start_time = time.time()\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        if is_batch:\n",
        "            print(f\"üöÄ Job {job_num}/{total_jobs}: {job_name}\")\n",
        "        else:\n",
        "            print(f\"üöÄ Running prediction: {job_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Setup job directory\n",
        "        job_dir = job_name\n",
        "        os.makedirs(job_dir, exist_ok=True)\n",
        "\n",
        "        # Create input FASTA file\n",
        "        input_file = os.path.join(job_dir, f\"{job_name}.fasta\")\n",
        "        with open(input_file, \"w\") as f:\n",
        "            for seq in sequences_data:\n",
        "                f.write(f\">{seq['id']}|{seq['type']}\\n{seq['content']}\\n\")\n",
        "\n",
        "        # Build Boltz command - ALWAYS include MSA server for reliability\n",
        "        cmd_parts = [\n",
        "            \"boltz\", \"predict\", input_file,\n",
        "            \"--out_dir\", job_dir,\n",
        "            \"--recycling_steps\", str(settings.get('recycling_steps', 3)),\n",
        "            \"--sampling_steps\", str(settings.get('sampling_steps', 200)),\n",
        "            \"--diffusion_samples\", str(settings.get('diffusion_samples', 5)),\n",
        "            \"--max_parallel_samples\", str(settings.get('max_parallel_samples', 5)),\n",
        "            \"--step_scale\", str(settings.get('step_scale', 1.638)),\n",
        "            \"--output_format\", settings.get('output_format', 'mmcif'),\n",
        "            \"--max_msa_seqs\", str(settings.get('max_msa_seqs', 8192)),\n",
        "            \"--override\"\n",
        "        ]\n",
        "\n",
        "        # ALWAYS add MSA server - this was the key missing piece!\n",
        "        if settings.get('use_msa_server', True):  # Default to True\n",
        "            cmd_parts.extend([\n",
        "                \"--use_msa_server\",\n",
        "                \"--msa_server_url\", settings.get('msa_server_url', 'https://api.colabfold.com'),\n",
        "                \"--msa_pairing_strategy\", settings.get('msa_pairing_strategy', 'greedy')\n",
        "            ])\n",
        "\n",
        "        # Add optional flags\n",
        "        if settings.get('use_potentials', True):\n",
        "            cmd_parts.append(\"--use_potentials\")\n",
        "        if settings.get('write_full_pae', True):\n",
        "            cmd_parts.append(\"--write_full_pae\")\n",
        "        if settings.get('write_full_pde', True):\n",
        "            cmd_parts.append(\"--write_full_pde\")\n",
        "        if settings.get('predict_affinity', False):\n",
        "            cmd_parts.extend([\n",
        "                \"--predict_affinity\",\n",
        "                \"--sampling_steps_affinity\", str(settings.get('sampling_steps_affinity', 200)),\n",
        "                \"--diffusion_samples_affinity\", str(settings.get('diffusion_samples_affinity', 5))\n",
        "            ])\n",
        "            if settings.get('affinity_mw_correction', False):\n",
        "                cmd_parts.append(\"--affinity_mw_correction\")\n",
        "\n",
        "        cmd = \" \".join(cmd_parts)\n",
        "        print(f\"üìã Command: {cmd}\")\n",
        "\n",
        "        # Run prediction\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                cmd,\n",
        "                shell=True,\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=7200\n",
        "            )\n",
        "\n",
        "            print(f\"Return code: {result.returncode}\")\n",
        "\n",
        "            if result.stdout:\n",
        "                print(\"STDOUT:\")\n",
        "                print(result.stdout[-1000:])\n",
        "\n",
        "            if result.stderr:\n",
        "                print(\"STDERR:\")\n",
        "                print(result.stderr[-500:])\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                # Check for failed examples in stdout\n",
        "                failed_examples = 0\n",
        "                if result.stdout and \"Number of failed examples:\" in result.stdout:\n",
        "                    import re\n",
        "                    match = re.search(r\"Number of failed examples:\\s*(\\d+)\", result.stdout)\n",
        "                    if match:\n",
        "                        failed_examples = int(match.group(1))\n",
        "\n",
        "                if failed_examples > 0:\n",
        "                    print(f\"‚ùå {failed_examples} prediction(s) failed - likely out of memory\")\n",
        "                    return False\n",
        "\n",
        "                # More comprehensive file detection\n",
        "                results_dirs = [d for d in os.listdir(job_dir) if d.startswith('boltz_results_')]\n",
        "                if results_dirs:\n",
        "                    predictions_dir = os.path.join(job_dir, results_dirs[0])\n",
        "                    print(f\"üìÅ Found results directory: {predictions_dir}\")\n",
        "\n",
        "                    # Search ALL subdirectories for structure files\n",
        "                    all_structure_files = []\n",
        "                    structure_extensions = ['.cif', '.pdb', '.mmcif', '.ent', '.cif.gz', '.pdb.gz']\n",
        "\n",
        "                    for root, dirs, files in os.walk(predictions_dir):\n",
        "                        for file in files:\n",
        "                            if any(file.endswith(ext) for ext in structure_extensions):\n",
        "                                full_path = os.path.join(root, file)\n",
        "                                rel_path = os.path.relpath(full_path, predictions_dir)\n",
        "                                all_structure_files.append(rel_path)\n",
        "                                print(f\"üß¨ Found structure file: {rel_path}\")\n",
        "\n",
        "                    # Also list ALL files for debugging\n",
        "                    print(f\"üìã All files in results directory:\")\n",
        "                    for root, dirs, files in os.walk(predictions_dir):\n",
        "                        level = root.replace(predictions_dir, '').count(os.sep)\n",
        "                        indent = '  ' * level\n",
        "                        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "                        subindent = '  ' * (level + 1)\n",
        "                        for file in files[:10]:  # Show first 10 files per directory\n",
        "                            print(f\"{subindent}{file}\")\n",
        "                        if len(files) > 10:\n",
        "                            print(f\"{subindent}... and {len(files) - 10} more files\")\n",
        "\n",
        "                    if not all_structure_files:\n",
        "                        print(f\"‚ùå No structure files found with extensions: {structure_extensions}\")\n",
        "                        return False\n",
        "                    else:\n",
        "                        print(f\"‚úÖ Found {len(all_structure_files)} structure file(s)\")\n",
        "\n",
        "                    # Create results zip\n",
        "                    zip_filename = f\"{job_name}_results.zip\"\n",
        "                    create_results_zip(job_dir, zip_filename)\n",
        "                    print(f\"üì¶ Created results archive: {zip_filename}\")\n",
        "\n",
        "                    # Upload to Google Drive if configured\n",
        "                    if global_settings.get('drive') and gdrive_folder_id:\n",
        "                        upload_url = upload_to_gdrive(global_settings['drive'], zip_filename, gdrive_folder_id, job_name)\n",
        "                        if upload_url:\n",
        "                            uploaded_files.append({'job': job_name, 'url': upload_url})\n",
        "\n",
        "                    # Cleanup\n",
        "                    try:\n",
        "                        shutil.rmtree(job_dir)\n",
        "                        if os.path.exists(zip_filename):\n",
        "                            os.remove(zip_filename)\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è  Cleanup warning: {e}\")\n",
        "\n",
        "                    job_duration = time.time() - job_start_time\n",
        "                    print(f\"‚úÖ Job completed in {job_duration:.1f}s\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"‚ùå No results directory found for {job_name}\")\n",
        "                    # Debug: show what directories DO exist\n",
        "                    existing_dirs = [d for d in os.listdir(job_dir) if os.path.isdir(os.path.join(job_dir, d))]\n",
        "                    print(f\"üìÅ Existing directories in {job_dir}: {existing_dirs}\")\n",
        "                    return False\n",
        "            else:\n",
        "                print(f\"‚ùå Prediction failed for {job_name}\")\n",
        "                return False\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"‚è∞ Prediction timed out for {job_name}\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"üí• Error running prediction for {job_name}: {e}\")\n",
        "            return False\n",
        "\n",
        "    # Setup Google Drive folder\n",
        "    gdrive_folder_id = None\n",
        "    uploaded_files = []\n",
        "\n",
        "    if global_settings.get('drive'):\n",
        "        gdrive_folder_id = find_or_create_folder(global_settings['drive'], global_settings['gdrive_folder_name'])\n",
        "\n",
        "    # Run predictions\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    if global_settings.get('batch_jobs'):\n",
        "        # Batch processing\n",
        "        batch_jobs = global_settings['batch_jobs']\n",
        "        print(f\"\\nüöÄ Starting batch processing of {len(batch_jobs)} jobs...\")\n",
        "        successful_jobs = 0\n",
        "\n",
        "        for i, job in enumerate(batch_jobs, 1):\n",
        "            success = run_single_prediction(\n",
        "                job['name'],\n",
        "                job['sequences'],\n",
        "                global_settings,\n",
        "                is_batch=True,\n",
        "                job_num=i,\n",
        "                total_jobs=len(batch_jobs)\n",
        "            )\n",
        "\n",
        "            if success:\n",
        "                successful_jobs += 1\n",
        "\n",
        "        print(f\"\\nüéâ Batch processing completed!\")\n",
        "        print(f\"‚úÖ Successful: {successful_jobs}/{len(batch_jobs)} jobs\")\n",
        "\n",
        "    elif global_settings.get('sequences'):\n",
        "        # Single job processing\n",
        "        print(f\"\\nüöÄ Starting single prediction...\")\n",
        "\n",
        "        # Convert sequences to expected format\n",
        "        single_job_sequences = []\n",
        "        for seq in global_settings['sequences']:\n",
        "            for chain_id in seq['chain_ids']:\n",
        "                single_job_sequences.append({\n",
        "                    'id': chain_id,\n",
        "                    'type': seq['type'],\n",
        "                    'content': seq['content']\n",
        "                })\n",
        "\n",
        "        if single_job_sequences:\n",
        "            success = run_single_prediction(\n",
        "                global_settings['final_jobname'],\n",
        "                single_job_sequences,\n",
        "                global_settings,\n",
        "                is_batch=False\n",
        "            )\n",
        "            if success:\n",
        "                print(f\"‚úÖ Single prediction completed successfully!\")\n",
        "            else:\n",
        "                print(f\"‚ùå Single prediction failed\")\n",
        "\n",
        "    # Summary\n",
        "    end_time = datetime.now()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üèÅ PREDICTION SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"‚è±Ô∏è  Total duration: {duration}\")\n",
        "    print(f\"üìÅ Google Drive folder: {global_settings['gdrive_folder_name']}\")\n",
        "\n",
        "    if uploaded_files:\n",
        "        print(f\"‚òÅÔ∏è  Files uploaded to Google Drive: {len(uploaded_files)}\")\n",
        "        for file_info in uploaded_files:\n",
        "            print(f\"   ‚Ä¢ {file_info['job']}: {file_info['url']}\")\n",
        "    else:\n",
        "        print(\"‚ùå No files were uploaded to Google Drive\")\n",
        "\n",
        "    print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSbCm2o1xpIz",
        "outputId": "ef12e9d6-f70e-45c5-d22b-e103e92a4855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GPU DIAGNOSIS ===\n",
            "PyTorch version: 2.7.1+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "GPU count: 1\n",
            "GPU name: NVIDIA A100-SXM4-40GB\n",
            "GPU memory: 42.5 GB\n",
            "‚úÖ GPU tensor operations working\n",
            "\n",
            "=== ENVIRONMENT CHECK ===\n",
            "CUDA_VISIBLE_DEVICES: Not set\n",
            "Working directory: /content\n",
            "\n",
            "=== BOLTZ VERSION INFO ===\n",
            "Boltz version stdout: \n",
            "Boltz version stderr: Usage: boltz [OPTIONS] COMMAND [ARGS]...\n",
            "Try 'boltz --help' for help.\n",
            "\n",
            "Error: No such option: --version\n",
            "\n",
            "\n",
            "=== BOLTZ HELP CHECK ===\n",
            "Boltz help (first 500 chars):\n",
            "Usage: boltz [OPTIONS] COMMAND [ARGS]...\n",
            "\n",
            "  Boltz.\n",
            "\n",
            "Options:\n",
            "  --help  Show this message and exit.\n",
            "\n",
            "Commands:\n",
            "  predict  Run predictions with Boltz.\n",
            "\n",
            "\n",
            "=== EXISTING DIRECTORIES CHECK ===\n",
            "Current directories: ['.config', 'sample_data']...\n",
            "Boltz-related directories: []\n",
            "\n",
            "=== MEMORY CHECK ===\n",
            "nvidia-smi output:\n",
            "Thu Jun 26 10:33:18 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             54W /  400W |     455MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "\n",
            "=== PYTHON PACKAGES CHECK ===\n",
            "torch: 2.6.0+cu124\n",
            "boltz: Not found\n",
            "pytorch-lightning: Not found\n",
            "torchmetrics: Not found\n",
            "\n",
            "=== CACHE DIRECTORY CHECK ===\n",
            "No cache dir: /root/.boltz\n",
            "No cache dir: /root/.boltz\n",
            "No cache dir: /tmp/.boltz\n",
            "No cache dir: ./.boltz\n",
            "\n",
            "=== MANUAL BOLTZ TEST ===\n",
            "Running a minimal Boltz test...\n",
            "Created test file: test_minimal.fasta\n",
            "Test command: boltz predict test_minimal.fasta --out_dir test_minimal_out --recycling_steps 1 --sampling_steps 10 --diffusion_samples 1\n",
            "Test return code: 0\n",
            "Test stdout (last 500 chars): a bit of time. You may change the cache directory with the --cache flag.\n",
            "Downloading the Boltz-2 weights to /root/.boltz/boltz2_conf.ckpt. You may change the cache directory with the --cache flag.\n",
            "Downloading the Boltz-2 affinity weights to /root/.boltz/boltz2_aff.ckpt. You may change the cache directory with the --cache flag.\n",
            "Checking input data.\n",
            "Processing 1 inputs with 1 threads.\n",
            "Failed to process test_minimal.fasta. Skipping. Error: Missing MSA's in input and --use_msa_server flag not set..\n",
            "\n",
            "Test stderr (last 300 chars): ^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Missing MSA's in input and --use_msa_server flag not set.\n",
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 78.59it/s]\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "\n",
            "Test output directory contents:\n",
            "test_minimal_out/\n",
            "  boltz_results_test_minimal/\n",
            "    predictions/\n",
            "    processed/\n",
            "      manifest.json\n",
            "      templates/\n",
            "      constraints/\n",
            "      records/\n",
            "      msa/\n",
            "      mols/\n",
            "      structures/\n",
            "    msa/\n",
            "\n",
            "=== DIAGNOSIS COMPLETE ===\n"
          ]
        }
      ],
      "source": [
        "#@title Debug GPU and Boltz Setup\n",
        "import torch\n",
        "import os\n",
        "import subprocess\n",
        "import glob\n",
        "\n",
        "print(\"=== GPU DIAGNOSIS ===\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Test GPU actually works\n",
        "    try:\n",
        "        test_tensor = torch.randn(1000, 1000).cuda()\n",
        "        result = torch.mm(test_tensor, test_tensor)\n",
        "        print(\"‚úÖ GPU tensor operations working\")\n",
        "        torch.cuda.empty_cache()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GPU test failed: {e}\")\n",
        "else:\n",
        "    print(\"‚ùå CUDA not available!\")\n",
        "\n",
        "print(\"\\n=== ENVIRONMENT CHECK ===\")\n",
        "print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "print(\"\\n=== BOLTZ VERSION INFO ===\")\n",
        "try:\n",
        "    result = subprocess.run([\"boltz\", \"--version\"], capture_output=True, text=True, timeout=10)\n",
        "    print(f\"Boltz version stdout: {result.stdout}\")\n",
        "    print(f\"Boltz version stderr: {result.stderr}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting Boltz version: {e}\")\n",
        "\n",
        "print(\"\\n=== BOLTZ HELP CHECK ===\")\n",
        "try:\n",
        "    result = subprocess.run([\"boltz\", \"--help\"], capture_output=True, text=True, timeout=10)\n",
        "    print(\"Boltz help (first 500 chars):\")\n",
        "    print(result.stdout[:500])\n",
        "except Exception as e:\n",
        "    print(f\"Error getting Boltz help: {e}\")\n",
        "\n",
        "print(\"\\n=== EXISTING DIRECTORIES CHECK ===\")\n",
        "current_dirs = [d for d in os.listdir('.') if os.path.isdir(d)]\n",
        "boltz_dirs = [d for d in current_dirs if 'boltz_results_' in d or any(f.startswith('boltz_') for f in os.listdir(d) if os.path.isdir(os.path.join(d, f)))]\n",
        "print(f\"Current directories: {current_dirs[:10]}...\")\n",
        "print(f\"Boltz-related directories: {boltz_dirs}\")\n",
        "\n",
        "if boltz_dirs:\n",
        "    for bdir in boltz_dirs[:3]:  # Check first 3\n",
        "        print(f\"\\n--- Examining {bdir} ---\")\n",
        "        for root, dirs, files in os.walk(bdir):\n",
        "            level = root.replace(bdir, '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files[:5]:  # Show first 5 files\n",
        "                print(f\"{subindent}{file}\")\n",
        "            if len(files) > 5:\n",
        "                print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "            if level > 3:  # Limit depth\n",
        "                break\n",
        "\n",
        "print(\"\\n=== MEMORY CHECK ===\")\n",
        "try:\n",
        "    result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True, timeout=10)\n",
        "    print(\"nvidia-smi output:\")\n",
        "    print(result.stdout)\n",
        "except Exception as e:\n",
        "    print(f\"Error running nvidia-smi: {e}\")\n",
        "\n",
        "print(\"\\n=== PYTHON PACKAGES CHECK ===\")\n",
        "import pkg_resources\n",
        "packages_of_interest = ['torch', 'boltz', 'pytorch-lightning', 'torchmetrics']\n",
        "for package in packages_of_interest:\n",
        "    try:\n",
        "        version = pkg_resources.get_distribution(package).version\n",
        "        print(f\"{package}: {version}\")\n",
        "    except:\n",
        "        print(f\"{package}: Not found\")\n",
        "\n",
        "print(\"\\n=== CACHE DIRECTORY CHECK ===\")\n",
        "cache_dirs = [\n",
        "    os.path.expanduser(\"~/.boltz\"),\n",
        "    \"/root/.boltz\",\n",
        "    \"/tmp/.boltz\",\n",
        "    \"./.boltz\"\n",
        "]\n",
        "for cache_dir in cache_dirs:\n",
        "    if os.path.exists(cache_dir):\n",
        "        print(f\"Found cache dir: {cache_dir}\")\n",
        "        try:\n",
        "            cache_files = os.listdir(cache_dir)\n",
        "            print(f\"  Files: {cache_files}\")\n",
        "        except:\n",
        "            print(f\"  Cannot list files in {cache_dir}\")\n",
        "    else:\n",
        "        print(f\"No cache dir: {cache_dir}\")\n",
        "\n",
        "print(\"\\n=== MANUAL BOLTZ TEST ===\")\n",
        "print(\"Running a minimal Boltz test...\")\n",
        "test_fasta_content = \"\"\">test|protein\n",
        "MKLLVLSLSLVLVVVSSQE\n",
        "\"\"\"\n",
        "\n",
        "test_file = \"test_minimal.fasta\"\n",
        "with open(test_file, \"w\") as f:\n",
        "    f.write(test_fasta_content)\n",
        "\n",
        "print(f\"Created test file: {test_file}\")\n",
        "\n",
        "# Run minimal Boltz command\n",
        "test_cmd = f\"boltz predict {test_file} --out_dir test_minimal_out --recycling_steps 1 --sampling_steps 10 --diffusion_samples 1\"\n",
        "print(f\"Test command: {test_cmd}\")\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(test_cmd, shell=True, capture_output=True, text=True, timeout=120)\n",
        "    print(f\"Test return code: {result.returncode}\")\n",
        "    print(f\"Test stdout (last 500 chars): {result.stdout[-500:]}\")\n",
        "    print(f\"Test stderr (last 300 chars): {result.stderr[-300:]}\")\n",
        "\n",
        "    # Check what was created\n",
        "    if os.path.exists(\"test_minimal_out\"):\n",
        "        print(\"\\nTest output directory contents:\")\n",
        "        for root, dirs, files in os.walk(\"test_minimal_out\"):\n",
        "            level = root.replace(\"test_minimal_out\", \"\").count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files:\n",
        "                print(f\"{subindent}{file}\")\n",
        "    else:\n",
        "        print(\"No test output directory created\")\n",
        "\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"Test command timed out\")\n",
        "except Exception as e:\n",
        "    print(f\"Test command failed: {e}\")\n",
        "\n",
        "# Cleanup\n",
        "try:\n",
        "    os.remove(test_file)\n",
        "    import shutil\n",
        "    if os.path.exists(\"test_minimal_out\"):\n",
        "        shutil.rmtree(\"test_minimal_out\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"\\n=== DIAGNOSIS COMPLETE ===\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMkiG2YlISVK/rrqD91deMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}